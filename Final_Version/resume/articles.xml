<articles>
  <article>
    <preamble>ACL2004-HEADLINE.txt</preamble>
    <titre>Hybrid Headlines: Combining Topics and Sentence Compression David Zajic, Bonnie Dorr, Stacy President Richard Schwartz Department of Computer Science BBN Technologies</titre>
    <auteur>University of Maryland 9861 Broken Land Parkway, Suite 156 College Park, MD 20742 Columbia, MD 21046 dmzajic,bonnie @umiacs.umd.edu schwartz@bbn.com stacypre@cs.umd.edu </auteur>
    <abstract>This paper presents Topiary, a headlinegeneration system that creates very short, informative summaries for news stories by combining sentence compression and unsupervised topic discovery. We will show that the combination of linguistically motivated sentence compression with statistically selected topic terms performs better than either alone, according to some automatic summary evaluation measures. In addition we describe experimental results establishing an appropriate extrinsic task on which to measure the effect of summarization on human performance. We demonstrate the usefulness of headlines in comparison to full texts in the context of this extrinsic task.</abstract>
    <biblio>Sabine Bergler, René Witte, Michelle Khalife, Zhuoyan Li, and Frank Rudzicz. 2003. Using knowledge-poor coreference resolution for text summarization. In Proceedings of the 2003 Document Understanding Conference, Draft Papers, pages 85– 92, Edmonton, Candada. Bonnie Dorr, David Zajic, and Richard Schwartz. 2003a. Cross-language headline generation for hindi. ACM Transactions on Asian Language Information Processing (TALIP), 2:2. Bonnie Dorr, David Zajic, and Richard Schwartz. 2003b. Hedge trimmer: A parse-and-trim approach to headline generation. In Proceedings of the HLTNAACL 2003 Text Summarization Workshop, Edmonton, Alberta, Canada, pages 1–8. T. Euler. 2002. Tailoring text using topic words: Selection and compression. In Proceedings of 13th International Workshop on Database and Expert Systems Applications (DEXA 2002), 2-6 September 2002, Aix-en-Provence, France, pages 215–222. IEEE Computer Society. Kevin Knight and Daniel Marcu. 2000. Statisticsbased summarization – step one: Sentence compression. In The 17th National Conference of the American Association for Artificial Intelligence AAAI2000, Austin, Texas. David Lewis. 1992. An evaluation of phrasal and clustered representations on a text categorization task. In Proceedings of the 15th annual international ACM SIGIR conference on Research and development in information retrieval, pages 37–50, Copenhagen, Denmark. Chin-Yew Lin and Eduard Hovy. 2003. Automatic Evaluation of Summaries Using N-gram CoOccurrences Statistics. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics, Edmonton, Alberta. Ingrid Mårdh. 1980. Headlinese: On the Grammar of English Front Page Headlines. Malmo. S. Miller, M. Crystal, H. Fox, L. Ramshaw, R. Schwartz, R. Stone, and R. Weischedel. 1998. Algorithms that Learn to Extract Information; BBN: Description of the SIFT System as Used for MUC-7. In Proceedings of the MUC-7. K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proceedings of Association of Computational Linguistics, Philadelphia, PA. R. Schwartz, T. Imai, F. Jubala, L. Nguyen, and J. Makhoul. 1997. A maximum likelihood model for topic classification of broadcast news. In Eurospeech-97, Rhodes, Greece. David Zajic, Bonnie Dorr, Richard Schwartz, and Stacy President. 2004. Headline evaluation experiment results, umiacs-tr-2004-18. Technical report, University of Maryland Institute for Advanced Computing Studies, College Park, Maryland. Liang Zhou and Eduard Hovy. 2003. Headline summarization at isi. In Proceedings of the 2003 Document Understanding Conference, Draft Papers, pages 174–178, Edmonton, Candada.</biblio>
  </article>
  <article>
    <preamble>Boudin-Torres-2006.txt</preamble>
    <titre>A Scalable MMR Approach to Sentence Scoring for Multi-Document Update Summarization Florian Boudin \ and Marc El-Bèze \ \ Laboratoire Informatique d’Avignon 339 chemin des Meinajaries, BP1228, 84911 Avignon Cedex 9, France. Juan-Manuel Torres-Moreno \,[ [ École Polytechnique de Montréal CP 6079 Succ. Centre Ville H3C 3A7 Montréal (Québec), Canada.</titre>
    <auteur>florian.boudin@univ-avignon.fr juan-manuel.torres@univ-avignon.fr marc.elbeze@univ-avignon.fr</auteur>
    <abstract>redundancy with previously read documents (history) has to be removed from the extract. A natural way to go about update summarization would be extracting temporal tags (dates, elapsed times, temporal expressions...) (Mani and Wilson, 2000) or to automatically construct the timeline from documents (Swan and Allan, 2000). These temporal marks could be used to focus extracts on the most recently written facts. However, most recently written facts are not necessarily new facts. Machine Reading (MR) was used by (Hickl et al., 2007) to construct knowledge representations from clusters of documents. Sentences containing “new” information (i.e. that could not be inferred by any previously considered document) are selected to generate summary. However, this highly efficient approach (best system in DUC 2007 update) requires large linguistic resources. (Witte et al., 2007) propose a rule-based system based on fuzzy coreference cluster graphs. Again, this approach requires to manually write the sentence ranking scheme. Several strategies remaining on post-processing redundancy removal techniques have been suggested. Extracts constructed from history were used by (Boudin and TorresMoreno, 2007) to minimize history’s redundancy. (Lin et al., 2007) have proposed a modified Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998) re-ranker during sentence selection, constructing the summary by incrementally re-ranking sentences. In this paper, we propose a scalable sentence scoring method for update summarization derived from MMR. Motivated by the need for relevant novelty, candidate sentences are selected according to a combined criterion of query relevance and dissimilarity with previously read sentences. The rest of the paper is organized as follows. Section 2 We present S MMR, a scalable sentence scoring method for query-oriented update summarization. Sentences are scored thanks to a criterion combining query relevance and dissimilarity with already read documents (history). As the amount of data in history increases, non-redundancy is prioritized over query-relevance. We show that S MMR achieves promising results on the DUC 2007 update corpus. 1</abstract>
    <biblio>Ye, S., L. Qiu, T.S. Chua, and M.Y. Kan. 2005. NUS at DUC 2005: Understanding documents via concept links. In Document Understanding Conference (DUC). Boudin, F. and J.M. Torres-Moreno. 2007. A Cosine Maximization-Minimization approach for UserOriented Multi-Document Update Summarization. In Recent Advances in Natural Language Processing (RANLP), pages 81–87. Carbonell, J. and J. Goldstein. 1998. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335–336. ACM Press New York, NY, USA. 26</biblio>
  </article>
  <article>
    <preamble>compression.txt</preamble>
    <titre>Multi-Candidate Reduction: Sentence Compression as a Tool for Document Summarization Tasks∗ David Zajic1 , Bonnie J. Dorr1 , Jimmy Lin1 , Richard Schwartz2 1</titre>
    <auteur>University of Maryland College Park, Maryland, USA dmzajic@cs.umd.edu, bonnie@umiacs.umd.edu, jimmylin@umd.edu 2 BBN Technologies 9861 Broken Land Parkway Columbia, MD 21046 schwartz@bbn.com</auteur>
    <abstract>This article examines the application of two single-document sentence compression techniques to the problem of multi-document summarization—a “parse-and-trim” approach and a statistical noisy-channel approach. We introduce the Multi-Candidate Reduction (MCR) framework for multi-document summarization, in which many compressed candidates are generated for each source sentence. These candidates are then selected for inclusion in the final summary based on a combination of static and dynamic features. Evaluations demonstrate that sentence compression is a valuable component of a larger multi-document summarization framework. Keywords: headline generation, summarization, parse-and-trim, Hidden Markov Model PACS: Artificial intelligence, 07.05.Mh; Computer science and technology, 89.20.Ff; Spoken languages, processing of, 43.71.Sy 1</abstract>
    <biblio>the same source sentences are available as candidates for inclusion in the final summary. Minimization of redundancy is an important element of a multi-document summarization system. Carbonell and Goldstein (1998) propose a technique called Maximal Marginal Relevance (MMR) for ranking documents returned by an information retrieval system so that the front of the ranked list will contain diversity as well as high relevance. Goldstein et al. (2000) extend MMR to multi-document summarization. MCR borrows the ranking approach of MMR, but uses a different set of features. Like MEAD, these approaches use feature weights that are optimized to maximize an automatic metric on training data. Several researchers have shown the importance of summarization in domains other than written news (Muresan et al., 2001; Clarke and Lapata, 2006). Within the MCR framework, we discuss the portability of Trimmer and HMM Hedge to a variety of different texts: written news, broadcast news transcriptions, email threads, and text in foreign language. 4 3 Single-Sentence Compression Our general approach to the generation of a summary from a single document is to produce a headline by selecting words in order from the text of the story. Consider the following excerpt from a news story and corresponding headline: (1) (i) (ii) After months of debate following the Sept. 11 terrorist hijackings, the Transportation Department has decided that airline pilots will not be allowed to have guns in the cockpits. Pilots not allowed to have guns in cockpits. The bold words in (1i) form a fluent and accurate headline, as shown in (1ii). This basic approach has been realized in two ways. The first, Trimmer, uses a linguisticallymotivated algorithm to remove grammatical constituents from the lead sentence until a length threshold is met. Topiary is a variant of Trimmer that combines fluent text from a compressed sentence with topic terms to produce headlines. The second, HMM Hedge, employs a noisy-channel model to find the most likely headline for a given story. The remainder of this section will present Trimmer, Topiary, and HMM Hedge in more detail. 3.1 Trimmer Our first approach to sentence compression involves iteratively removing grammatical constituents from the parse tree of a sentence using linguistically-motivated rules until a length threshold has been met. When applied to the lead sentence, or first non-trivial sentence of a story, our algorithm generates a very short summary, or headline. This idea is implemented in our Trimmer system, which can leverage the output of any constituency parser that uses the Penn Treebank conventions. At present we use Charniak’s parser (Charniak, 2000). The insights that form the basis and justification for the Trimmer rules come from our previous study, which compared the relative prevalence of certain constructions in human-written summaries and lead sentences in stories. This study used 218 human-written summaries of 73 documents from the TIPSTER corpus (Harman and Liberman, 1993) dated January 1, 1989. The 218 summaries and the lead sentences of the 73 stories were parsed using the BBN SIFT parser (Miller et al., 2000). The parser produced 957 noun phrases (NP nodes in the parse trees) and 315 clauses (S nodes in the parse trees) for the 218 summaries. For the 73 lead sentences, the parser produced 817 noun phrases and 316 clauses. At each level (sentence, clause, and noun phrase), different types of linguistic phenomena were counted. • At the sentence level, the numbers of preposed adjuncts, conjoined clauses, and conjoined verb phrases were counted. Children of the root S node that occur to the left of the first NP are considered to be preposed adjuncts. The bracketed phase in “[According to police] the crime rate has gone down” is a prototypical example of a preposed adjunct. • At the clause level, temporal expressions, trailing SBAR nodes, and trailing PP nodes were counted. Trailing constituents are those not designated as an argument of a verb phrase. • At both the sentence and clause levels, conjoined S nodes and conjoined VP nodes were counted. • At the NP level, determiners and relative clauses were counted. The counts and prevalence of the phenomena in the human-generated headlines and lead sentences are shown in Table 1. The results of this analysis illuminated the opportunities for trimming constituents and guided the development of our Trimmer rules, detailed below. 5 Level Sentence Clause Noun Phrase Phenomenon preposed adjuncts conjoined S conjoined VP temporal expression trailing PP trailing SBAR relative clause determiner Summary 0/218 0% 1/218 0.5% 7/218 3% 5/315 L. Bahl, F. Jelinek, and R. Mercer. 1983. A maximum likelihood approach to continuous speech recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 5(2):179–190. M. Banko, V. Mittal, and M. Witbrock. 2000. Headline generation based on statistical translation. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL 2000), pages 318–325, Hong Kong. R. Barzilay, N. Elhadad, and K. McKeown. 2002. Inferring strategies for sentence ordering in multidocument news summarization. Journal of Artificial Intelligence Research, 17:35–55. L. Baum. 1972. An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process. Inequalities, 3:1–8. S. Bergler, R. Witte, M. Khalife, Z. Li, and F. Rudzicz. 2003. Using knowledge-poor coreference resolution for text summarization. In Proceedings of the HLT-NAACL 2003 Text Summarization Workshop and Document Understanding Conference (DUC 2003), pages 85–92, Edmonton, Alberta. D. Bikel, R. Schwartz, and R. Weischedel. 1999. An algorithm that learns what’s in a name. Machine Learning, 34(1/3):211–231. S. Blair-Goldensohn, D. Evans, V. Hatzivassiloglou, K. McKeown, A. Nenkova, R. Passonneau, B. Schiffman, A. Schlaikjer, A. Siddharthan, and S. Siegelman. 2004. Columbia University at DUC 2004. In Proceedings of the 2004 Document Understanding Conference (DUC 2004) at HLT/NAACL 2004, pages 23–30, Boston, Massachusetts. 24 P. Brown, J. Cocke, S. Pietra, V. Pietra, F. Jelinek, J. Lafferty, R. Mercer, and P. Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):79–85. J. Carbonell and J. Goldstein. 1998. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1998), pages 335–336, Melbourne, Australia. Eugene Charniak. 2000. A Maximum-Entropy-Inspired Parser. In Proceedings of the First Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL 2000), pages 132–139, Seattle, Washington. J. Clarke and M. Lapata. 2006. Models for sentence compression: A comparison across domains, training requirements and evaluation measures. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL 2006), pages 377–384, Sydney, Australia. J. Conroy, J. Schlesinger, and J. Goldstein. 2005. CLASSY query-based multi-document summarization. In Proceedings of the 2005 Document Understanding Conference (DUC-2005) at NLT/EMNLP 2005, Vancouver, Canada. J. Conroy, J. Schlesinger, D. O’Leary, and J. Goldstein. 2006. Back to basics: CLASSY 2006. In Proceedings of the 2006 Document Understanding Conference (DUC 2006) at HLT/NAACL 2006, New York, New York. D. Cutting, J. Pedersen, and P. Sibun. 1992. A practical part-of-speech tagger. In Proceedings of the Third Conference on Applied Natural Language Processing, Trento, Italy. Hoa Dang and Donna Harman. 2006. Proceedings of the 2006 Document Understanding Conference (DUC 2006) at HLT/NAACL 2006. B. Dorr and T. Gaasterland. this special issue, 2007. Exploiting aspectual features and connecting words for summarization-inspired temporal-relation extraction. Information Processing and Management. B. Dorr, D. Zajic, and R. Schwartz. 2003a. Cross-language headline generation for Hindi. ACM Transactions on Asian Language Information Processing (TALIP), 2(3):270–289. B. Dorr, D. Zajic, and R. Schwartz. 2003b. Hedge Trimmer: A parse-and-trim approach to headline generation. In Proceedings of the HLT-NAACL 2003 Text Summarization Workshop and Document Understanding Conference (DUC 2003), pages 1–8, Edmonton, Alberta. T. Dunning. 1994. Statistical identification of language. Technical Report MCCS 94-273, New Mexico State University. T. Euler. 2002. Tailoring text using topic words: Selection and compression. In Proceedings of 13th International Workshop on Database and Expert Systems Applications (DEXA 2002), pages 215– 222, Aix-en-Provence, France. J. Goldstein, V. Mittal, J. Carbonell, and M. Kantrowitz. 2000. Multi-document summarization by sentence extraction. In Proceedings of ANLP/NAACL 2000 Workshop on Automatic Summarization, pages 40–48. D. Harman and M. Liberman. 1993. TIPSTER Complete. Linguistic Data Consortium (LDC), Philadelphia. 25 H. Jing and K. McKeown. 2000. Cut and paste based text summarization. In Proceedings of the First Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL 2000), pages 178–185, Seattle, Washington. B. Klimt and Y. Yang. 2004. Introducing the Enron Corpus. In Proceedings of the First Conference on Email and Anti-Spam (CEAS), Mountain View, California. K. Knight and D. Marcu. 2000. Statistics-based summarization—step one: Sentence compression. In Proceedings of the Seventeenth National Conference on Artificial Intelligence (AAAI-2000), Austin, Texas. K. Knight and D. Marcu. 2002. Summarization beyond sentence extraction: A probabilistic approach to sentence compression. Artificial Intelligence, 139(1):91–107. M. Lapata. 2003. Probabilistic text structuring: Experiments with sentence ordering. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL 2004), pages 545–552, Barcelona, Spain. David Dolan Lewis. 1999. An evaluation of phrasal and clustered representations on a text categorization task. In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1992), pages 37–50, Copenhagen, Denmark. C.-Y. Lin and E. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of the 2003 Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT/NAACL 2003), pages 71–78, Edmonton, Alberta. I. Mårdh. 1980. Headlinese: On the Grammar of English Front Page Headlines. Malmo. E. Mays, F. Damerau, and R. Mercer. 1990. Context-based spelling correction. In Proceedings of IBM Natural Language ITL, pages 517–522, Paris, France. S. Miller, L. Ramshaw, H. Fox, and R. Weischedel. 2000. A novel use of statistical parsing to extract information from text. In Proceedings of the First Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL 2000), pages 226–233, Seattle, Washington. S. Muresan, E. Tzoukermann, and J. Klavans. 2001. Combining linguistic and machine learning techniques for email. In Proceedings of the ACL/EACL 2001 Workshop on Computational Natural Language Learning (ConLL), pages 290–297, Toulouse, France. N. Okazaki, Y. Matsuo, and M. Ishizuka. 2004. Improving chronological sentence ordering by precedence relation. In Proceedings of the 20th International Conference on Computational Linguistics (COLING 2004), pages 750–756, Geneva, Switzerland. M. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137. D. Radev, T. Allison, S. Blair-Goldensohn, J. Blitzer, A. Çelebi, S. Dimitrov, E. Drabek, A. Hakim, W. Lam, D. Liu, J. Otterbacher, H. Qi, H. Saggion, S. Teufel, M. Topper, A. Winkel, and Z. Zhang. 2004. MEAD—a platform for multidocument multilingual text summarization. In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC 2004), Lisbon, Portugal. 26 R. Schwartz, T. Imai, F. Jubala, L. Nguyen, and J. Makhoul. 1997. A maximum likelihood model for topic classification of broadcast news. In Proceedings of the Fifth European Speech Communication Association Conference on Speech Communication and Technology (Eurospeech-97), Rhodes, Greece. S. Sista, R. Schwartz, T. Leek, and J. Makhoul. 2002. An algorithm for unsupervised topic discovery from broadcast news stories. In Proceedings of the 2002 Human Language Technology Conference (HLT), pages 99–103, San Diego, California. J. Turner and E. Charniak. 2005. Supervised and unsupervised learning for sentence compression. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pages 290–297, Ann Arbor, Michigan. L. Vanderwende, H. Suzuki, and C. Brockett. 2006. Microsoft Research at DUC2006: Task-focused summarization with sentence simplification and lexical expansion. In Proceedings of the 2006 Document Understanding Conference (DUC 2006) at HLT/NAACL 2006, New York, New York. A. Viterbi. 1967. Error bounds for convolution codes and an asymptotically optimal decoding algorithm. IEEE Transactions on Information Theory, 13:260–269. R. Wang, N. Stokes, W. Doran, E. Newman, J. Carthy, and J. Dunnion. 2005. Comparing Topiarystyle approaches to headline generation. In Lecture Notes in Computer Science: Advances in Information Retrieval: 27th European Conference on IR Research (ECIR 2005), volume 3408, Santiago de Compostela, Spain. Springer Berlin / Heidelberg. D. Zajic, B. Dorr, and R. Schwartz. 2004. BBN/UMD at DUC-2004: Topiary. In Proceedings of the 2004 Document Understanding Conference (DUC 2004) at NLT/NAACL 2004, pages 112–119, Boston, Massachusetts. D. Zajic, B. Dorr, J. Lin, and R. Schwartz. 2005a. UMD/BBN at MSE2005. In Proceedings of the MSE2005 Track of the ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, Ann Arbor, Michigan. D. Zajic, B. Dorr, R. Schwartz, C. Monz, and J. Lin. 2005b. A sentence-trimming approach to multi-document summarization. In Proceedings of the 2005 Document Understanding Conference (DUC-2005) at NLT/EMNLP 2005, pages 151–158, Vancouver, Canada. D. Zajic. 2007. Multiple Alternative Sentence Compressions (MASC) as a Tool for Automatic Summarization Tasks. Ph.D. thesis, University of Maryland, College Park. L. Zhou and E. Hovy. 2003. Headline summarization at ISI. In Proceedings of the HLT-NAACL 2003 Text Summarization Workshop and Document Understanding Conference (DUC 2003), pages 174–178, Edmonton, Alberta. 27</biblio>
  </article>
  <article>
    <preamble>compression_phrases_Prog-Linear-jair.txt</preamble>
    <titre>Journal of Artificial Intelligence Research 31 (2008) 399-429 Submitted 09/07; published 03/08 Global Inference for Sentence Compression An Integer Linear Programming Approach James Clarke</titre>
    <auteur>jclarke@ed.ac.uk Mirella Lapata mlap@inf.ed.ac.uk School of Informatics University of Edinburgh 2 Buccleuch Place Edinburgh EH8 9LW, UK</auteur>
    <abstract>Sentence compression holds promise for many applications ranging from summarization to subtitle generation. Our work views sentence compression as an optimization problem and uses integer linear programming (ILP) to infer globally optimal compressions in the presence of linguistically motivated constraints. We show how previous formulations of sentence compression can be recast as ILPs and extend these models with novel global constraints. Experimental results on written and spoken texts demonstrate improvements over state-of-the-art models. sentences had to contain a subset of the source sentence’s words and the word order had to remain the same. In earlier work (Clarke & Lapata, 2006) we have argued that the Ziff-Davis corpus is not ideal for studying compression for several reasons. First, we showed that human-authored compressions differ substantially from the Ziff-Davis which tends to be more aggressively compressed. Second, humans are more likely to drop individual words than lengthy constituents. Third, the test portion of the Ziff-Davis contains solely 32 sentences. This is an extremely small data set to reveal any statistically significant differences among systems. In fact, previous studies relied almost exclusively on human judgments for assessing the well-formedness of the compressed output, and significance tests are reported for by-subjects analyses only. We thus focused in the present study on manually created corpora. Specifically, we asked annotators to perform sentence compression by removing tokens on a sentence-bysentence basis. Annotators were free to remove any words they deemed superfluous provided their deletions: (a) preserved the most important information in the source sentence, and (b) ensured the compressed sentence remained grammatical. If they wished, they could leave a sentence uncompressed by marking it as inappropriate for compression. They were not allowed to delete whole sentences even if they believed they contained no information content guidelines, our annotators produced compressions of 82 newspaper articles (1,433 sentences) from the British National Corpus (BNC) and the American News Text corpus (henceforth written corpus) and 50 stories (1,370 sentences) from the HUB-4 1996 English Broadcast News corpus (henceforth spoken corpus). The written corpus contains articles from The LA 419 Clarke & Lapata Times, Washington Post, Independent, The Guardian and Daily Telegraph. The spoken corpus contains broadcast news from a variety of networks (CNN, ABC, CSPAN and NPR) which have been manually transcribed and segmented at the story and sentence level. Both corpora have been split into training, development and testing sets6 randomly on article boundaries (with each set containing full stories) and are publicly available from http: //homepages.inf.ed.ac.uk/s0460084/data/. 4.2 Parameter Estimation In this work we present three compression models ranging from unsupervised to semisupervised, and fully supervised. The unsupervised model simply relies on a trigram language model for driving compression (see Section 3.4.1). This was estimated from 25 million tokens of the North American corpus using the CMU-Cambridge Language Modeling Toolkit (Clarkson & Rosenfeld, 1997) with a vocabulary size of 50,000 tokens and GoodTuring discounting. To discourage one-word output we force the ILP to generate compressions whose length is no less than 40% of the source sentence (see the constraint in (9)). The semi-supervised model is the weighted combination of a word-based significance score with a language model (see Section 3.4.2). The significance score was calculated using 25 million tokens from the American News Text corpus. We optimized its weight (see Equation (11)) on a small subset of the training data (three documents in each case) using Powell’s method (Press, Teukolsky, Vetterling, & Flannery, 1992) and a loss function based on the F-score of the grammatical relations found in the gold standard compression and the system’s best compression (see Section 4.3 for details). The optimal weight was approximately 1.8 for the written corpus and 2.2 for the spoken corpus. McDonald’s (2006) supervised model was trained on the written and spoken training sets. Our implementation used the same feature sets as McDonald, the only difference being that our phrase structure and dependency features were extracted from the output of Roark’s (2001) parser. McDonald uses Charniak’s (2000) parser which performs comparably. The model was learnt using k-best compressions. On the development data, we found that k = 10 provided the best performance. 4.3 Evaluation Previous studies have relied almost exclusively on human judgments for assessing the wellformedness of automatically derived compressions. These are typically rated by naive subjects on two dimensions, grammaticality and importance (Knight & Marcu, 2002). Although automatic evaluation measures have been proposed (Riezler et al., 2003; Bangalore, Rambow, & Whittaker, 2000) their use is less widespread, we suspect due to the small size of the test portion of the Ziff-Davis corpus which is commonly used in compression work. We evaluate the output of our models in two ways. First, we present results using an automatic evaluation measure put forward by Riezler et al. (2003). They compare the grammatical relations found in the system compressions against those found in a gold standard. This allows us to measure the semantic aspects of summarization quality in terms of grammatical-functional information and can be quantified using F-score. Furthermore, 6. The splits are 908/63/462 sentences for the written corpus and 882/78/410 sentences for the spoken corpus. 420 Global Inference for Sentence Compression in Clarke and Lapata (2006) we show that relations-based F-score correlates reliably with human judgments on compression output. Since our test corpora are larger than ZiffDavis (by more than a factor of ten), differences among systems can be highlighted using significance testing. Our implementation of the F-score measure used the grammatical relations annotations provided by RASP (Briscoe & Carroll, 2002). This parser is particularly appropriate for the compression task since it provides parses for both full sentences and sentence fragments and is generally robust enough to analyze semi-grammatical sentences. We calculated F-score over all the relations provided by RASP (e.g., subject, direct/indirect object, modifier; 15 in total). In line with previous work we also evaluate our models by eliciting human judgments. Following the work of Knight and Marcu (2002), we conducted two separate experiments. In the first experiment participants were presented with a source sentence and its target compression and asked to rate how well the compression preserved the most important information from the source sentence. In the second experiment, they were asked to rate the grammaticality of the compressed outputs. In both cases they used a five point rating scale where a high number indicates better performance. We randomly selected 21 sentences from the test portion of each corpus. These sentences were compressed automatically by the three models presented in this paper with and without constraints. We also included gold standard compressions. Our materials thus consisted of 294 (21 × 2 × 7) sourcetarget sentences. A Latin square design ensured that subjects did not see two different compressions of the same sentence. We collected ratings from 42 unpaid volunteers, all self reported native English speakers. Both studies were conducted over the Internet using a custom build web interface. Examples of our experimental items are given in Table 3. 5. Results Let us first discuss our results when compression output is evaluated in terms of F-score. Tables 4 and 5 illustrate the performance of our models on the written and spoken corpora, respectively. We also present the compression rate7 for each system. In all cases the constraint-based models (+Constr) yield better F-scores than the non-constrained ones. The difference is starker for the semi-supervised model (Sig). The constraints bring an improvement of 17.2% on the written corpus and 18.3% on the spoken corpus. We further examined whether performance differences among models are statistically significant, using the Wilcoxon test. On the written corpus all constraint models significantly outperform the models without constraints. The same tendency is observed on the spoken corpus except for the model of McDonald (2006) which performs comparably with and without constraints. We also wanted to establish which is the best constraint model. On both corpora we find that the language model performs worst, whereas the significance model and McDonald perform comparably (i.e., the F-score differences are not statistically significant). To get a feeling for the difficulty of the task, we calculated how much our annotators agreed in their compression output. The inter-annotator agreement (F-score) on the written corpus was 65.8% and on the spoken corpus 73.4%. The agreement is higher on spoken texts since they consists of many short utterances (e.g., Okay, That’s it for now, Good night) that can 7. The term refers to the percentage of words retained from the source sentence in the compression. 421 Clarke & Lapata Source The aim is to give councils some control over the future growth of second homes. Gold The aim is to give councils control over the growth of homes. LM The aim is to the future. LM+Constr The aim is to give councils control. Sig The aim is to give councils control over the future growth of homes. Sig+Constr The aim is to give councils control over the future growth of homes. McD The aim is to give councils. McD+Constr The aim is to give councils some control over the growth of homes. Source The Clinton administration recently unveiled a new means to encourage brownfields redevelopment in the form of a tax incentive proposal. Gold The Clinton administration unveiled a new means to encourage brownfields redevelopment in a tax incentive proposal. LM The Clinton administration in the form of tax. LM+Constr The Clinton administration unveiled a means to encourage redevelopment in the form. Sig The Clinton administration unveiled a encourage brownfields redevelopment form tax proposal. Sig+Constr The Clinton administration unveiled a means to encourage brownfields redevelopment in the form of tax proposal. McD The Clinton unveiled a means to encourage brownfields redevelopment in a tax incentive proposal. McD+Constr The Clinton administration unveiled a means to encourage brownfields redevelopment in the form of a incentive proposal. Table 3: Example compressions produced by our systems (Source: source sentence, Gold: gold-standard compression, LM: language model compression, LM+Constr: language model compression with constraints, Sig: significance model, Sig+Constr: significance model with constraints, McD: McDonald’s (2006) compression model, McD+Constr: McDonald’s (2006) compression model with constraints). be compressed only very little or not all. Note that there is a marked difference between the automatic and human compressions. Our best performing systems are inferior to human output by more than 20 F-score percentage points. Differences between the automatic systems and the human output are also observed with respect to the compression rate. As can be seen the language model compresses most aggressively, whereas the significance model and McDonald tend to be more conservative and closer to the gold standard. Interestingly, the constraints do not necessarily increase the compression rate. The latter increases for the significance model but decreases for the language model and remains relatively constant for McDonald. It is straightforward to impose the same compression rate for all constraint-based models (e.g., by forcing the model P to retain b tokens ni=1 δi = b). However, we refrained from doing this since we wanted our 422 Global Inference for Sentence Compression Models LM Sig McD LM+Constr Sig+Constr McD+Constr Gold CompR 46.2 60.6 60.1 41.2 72.0 63.7 70.3 F-score 18.4 23.3 36.0 28.2∗ 40.5∗† 40.8∗† — Table 4: Results on the written corpus; compression rate (CompR) and grammatical relation F-score (F-score); ∗ : +Constr model is significantly different from model without constraints; † : significantly different from LM+Constr. Models LM Sig McD LM+Constr Sig+Constr McD+Constr Gold CompR 52.0 60.9 68.6 49.5 78.4 68.5 76.1 F-score 25.4 30.4 47.6 34.8∗ 48.7∗† 50.1† — Table 5: Results on the spoken corpus; compression rate (CompR) and grammatical relation F-score (F-score); ∗ : +Constr model is significantly different from without constraints; † : significantly different from LM+Constr. models to regulate the compression rate for each sentence individually according to its specific information content and structure. We next consider the results of our human study which assesses in more detail the quality of the generated compressions on two dimensions, namely grammaticality and information content. F-score conflates these two dimensions and therefore in theory could unduly reward a system that produces perfectly grammatical output without any information loss. Tables 6 and 7 show the mean ratings8 for each system (and the gold standard) on the written and spoken corpora, respectively. We first performed an Analysis of Variance (Anova) to examine the effect of different system compressions. The Anova revealed a reliable effect on both grammaticality and importance for each corpus (the effect was significant by both subjects and items (p < 0.01)). We next examine the impact of the constraints (+Constr in the tables). In most cases we observe an increase in ratings for both grammaticality and importance when a model is supplemented constraints. Post-hoc Tukey tests reveal that the grammaticality and importance ratings of the language model and significance model significantly improve with 8. All statistical tests reported subsequently were done using the mean ratings. 423 Clarke & Lapata Models Grammar Importance LM Sig McD 2.25†$ 3.05†</abstract>
    <biblio>Aho, A. V., & Ullman, J. D. (1969). Syntax directed translations and the pushdown assembler. Journal of Computer and System Sciences, 3, 37–56. Bangalore, S., Rambow, O., & Whittaker, S. (2000). Evaluation metrics for generation. In Proceedings of the first International Conference on Natural Language Generation, pp. 1–8, Mitzpe Ramon, Israel. Barzilay, R., & Lapata, M. (2006). Aggregation via set partitioning for natural language generation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pp. 359– 366, New York, NY, USA. Bramsen, P., Deshpande, P., Lee, Y. K., & Barzilay, R. (2006). Inducing temporal graphs. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pp. 189–198, Sydney, Australia. Briscoe, E. J., & Carroll, J. (2002). Robust accurate statistical annotation of general text. In Proceedings of the Third International Conference on Language Resources and Evaluation, pp. 1499–1504, Las Palmas, Gran Canaria. Charniak, E. (2000). A maximum-entropy-inspired parser. In Proceedings of the 1st North American Annual Meeting of the Association for Computational Linguistics, pp. 132– 139, Seattle, WA, USA. Clarke, J., & Lapata, M. (2006). Models for sentence compression: A comparison across domains, training requirements and evaluation measures. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pp. 377–384, Sydney, Australia. Clarkson, P., & Rosenfeld, R. (1997). Statistical language modeling using the CMU– Cambridge toolkit. In Proceedings of Eurospeech’97, pp. 2707–2710, Rhodes, Greece. 426 Global Inference for Sentence Compression Cormen, T. H., Leiserson, C. E., & Rivest, R. L. (1992). Intoduction to Algorithms. The MIT Press. Corston-Oliver, S. (2001). Text Compaction for Display on Very Small Screens. In Proceedings of the Workshop on Automatic Summarization at the 2nd Meeting of the North American Chapter of the Association for Computational Linguistics, pp. 89–98, Pittsburgh, PA, USA. Crammer, K., & Singer, Y. (2003). Ultraconservative online algorithms for multiclass problems. Journal of Machine Learning Research, 3, 951–991. Dantzig, G. B. (1963). Linear Programming and Extensions. Princeton University Press, Princeton, NJ, USA. Denis, P., & Baldridge, J. (2007). Joint determination of anaphoricity and coreference resolution using integer programming. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pp. 236–243, Rochester, NY. Dras, M. (1999). Tree Adjoining Grammar and the Reluctant Paraphrasing of Text. Ph.D. thesis, Macquarie University. Galley, M., & McKeown, K. (2007). Lexicalized markov grammars for sentence compression. In In Proceedings of the North American Chapter of the Association for Computational Linguistics, pp. 180–187, Rochester, NY, USA. Gomory, R. E. (1960). Solving linear programming problems in integers. In Bellman, R., & Hall, M. (Eds.), Combinatorial analysis, Proceedings of Symposia in Applied Mathematics, Vol. 10, Providence, RI, USA. Grefenstette, G. (1998). Producing Intelligent Telegraphic Text Reduction to Provide an Audio Scanning Service for the Blind. In Hovy, E., & Radev, D. R. (Eds.), Proceedings of the AAAI Symposium on Intelligent Text Summarization, pp. 111–117, Stanford, CA, USA. Hori, C., & Furui, S. (2004). Speech summarization: an approach through word extraction and a method for evaluation. IEICE Transactions on Information and Systems, E87D (1), 15–25. Jing, H. (2000). Sentence reduction for automatic text summarization. In Proceedings of the 6th Applied Natural Language Processing Conference, pp. 310–315, Seattle,WA, USA. Knight, K., & Marcu, D. (2002). Summarization beyond sentence extraction: a probabilistic approach to sentence compression. Artificial Intelligence, 139 (1), 91–107. Land, A. H., & Doig, A. G. (1960). An automatic method for solving discrete programming problems. Econometrica, 28, 497–520. Lin, C.-Y. (2003). Improving summarization performance by sentence compression — a pilot study. In Proceedings of the 6th International Workshop on Information Retrieval with Asian Languages, pp. 1–8, Sapporo, Japan. Lin, D. (2001). LaTaT: Language and text analysis tools. In Proceedings of the first Human Language Technology Conference, pp. 222–227, San Francisco, CA, USA. 427 Clarke & Lapata Marciniak, T., & Strube, M. (2005). Beyond the pipeline: Discrete optimization in NLP. In Proceedings of the Ninth Conference on Computational Natural Language Learning, pp. 136–143, Ann Arbor, MI, USA. McDonald, R. (2006). Discriminative sentence compression with soft syntactic constraints. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, Trento, Italy. McDonald, R., Crammer, K., & Pereira, F. (2005a). Flexible text segmentation with structured multilabel classification. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pp. 987–994, Vancouver, BC, Canada. McDonald, R., Crammer, K., & Pereira, F. (2005b). Online large-margin training of dependency parsers. In 43rd Annual Meeting of the Association for Computational Linguistics, pp. 91–98, Ann Arbor, MI, USA. Nemhauser, G. L., & Wolsey, L. A. (1988). Integer and Combinatorial Optimization. WileyInterscience series in discrete mathematicals and opitmization. Wiley, New York, NY, USA. Nguyen, M. L., Shimazu, A., Horiguchi, S., Ho, T. B., & Fukushi, M. (2004). Probabilistic sentence reduction using support vector machines. In Proceedings of the 20th international conference on Computational Linguistics, pp. 743–749, Geneva, Switzerland. Press, W. H., Teukolsky, S. A., Vetterling, W. T., & Flannery, B. P. (1992). Numerical Recipes in C: The Art of Scientific Computing. Cambridge University Press, New York, NY, USA. Punyakanok, V., Roth, D., Yih, W., & Zimak, D. (2004). Semantic role labeling via integer linear programming inference. In Proceedings of the International Conference on Computational Linguistics, pp. 1346–1352, Geneva, Switzerland. Riedel, S., & Clarke, J. (2006). Incremental integer linear programming for non-projective dependency parsing. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pp. 129–137, Sydney, Australia. Riezler, S., King, T. H., Crouch, R., & Zaenen, A. (2003). Statistical sentence condensation using ambiguity packing and stochastic disambiguation methods for lexical-functional grammar. In Human Language Technology Conference and the 3rd Meeting of the North American Chapter of the Association for Computational Linguistics, pp. 118– 125, Edmonton, Canada. Roark, B. (2001). Probabilistic top-down parsing and language modeling. Computational Linguistics, 27 (2), 249–276. Roth, D. (1998). Learning to resolve natural language ambiguities: A unified approach. In In Proceedings of the 15th of the American Association for Artificial Intelligence, pp. 806–813, Madison, WI, USA. Roth, D., & Yih, W. (2004). A linear programming formulation for global inference in natural language tasks. In Proceedings of the Annual Conference on Computational Natural Language Learning, pp. 1–8, Boston, MA, USA. 428 Global Inference for Sentence Compression Roth, D., & Yih, W. (2005). Integer linear programming inference for conditional random fields. In Proceedings of the International Conference on Machine Learning, pp. 737– 744, Bonn. Sarawagi, S., & Cohen, W. W. (2004). Semi-markov conditional random fields for information extraction. In Advances in Neural Information Processing Systems, Vancouver, BC, Canada. Shieber, S., & Schabes, Y. (1990). Synchronous tree-adjoining grammars. In Proceedings of the 13th International Conference on Computational Linguistics, pp. 253–258, Helsinki, Finland. Turner, J., & Charniak, E. (2005). Supervised and unsupervised learning for sentence compression. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pp. 290–297, Ann Arbor, MI, USA. Vandeghinste, V., & Pan, Y. (2004). Sentence compression for automated subtitling: A hybrid approach. In Marie-Francine Moens, S. S. (Ed.), Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pp. 89–95, Barcelona, Spain. Williams, H. P. (1999). Model Building in Mathematical Programming (4th edition). Wiley. Winston, W. L., & Venkataramanan, M. (2003). Introduction to Mathematical Programming: Applications and Algorithms (4th edition). Duxbury. Zajic, D., Door, B. J., Lin, J., & Schwartz, R. (2007). Multi-candidate reduction: Sentence compression as a tool for document summarization tasks. Information Processing Management Special Issue on Summarization, 43 (6), 1549–1570. 429</biblio>
  </article>
  <article>
    <preamble>hybrid_approach.txt</preamble>
    <titre>Sentence Compression for Automated Subtitling: A Hybrid Approach Vincent Vandeghinste and Yi Pan Centre for Computational Linguistics Katholieke Universiteit Leuven Maria Theresiastraat 21 BE-3000 Leuven Belgium</titre>
    <auteur>vincent.vandeghinste@ccl.kuleuven.ac.be, yi.pan@ccl.kuleuven.ac.be</auteur>
    <abstract>In this paper a sentence compression tool is described. We describe how an input sentence gets analysed by using a.o. a tagger, a shallow parser and a subordinate clause detector, and how, based on this analysis, several compressed versions of this sentence are generated, each with an associated estimated probability. These probabilities were estimated from a parallel transcript/subtitle corpus. To avoid ungrammatical sentences, the tool also makes use of a number of rules. The evaluation was done on three different pronunciation speeds, averaging sentence reduction rates of 40% to 17%. The number of reasonable reductions ranges between 32.9% and 51%, depending on the average estimated pronunciation speed.</abstract>
    <biblio>G. Booij and A. van Santen. 1995. Morfologie. De woordstructuur van het Nederlands. Amsterdam University Press, Amsterdam, Netherlands. T. Brants. 2000. TnT - A Statistical Part-of-Speech Tagger. Published online at http://www.coli.unisb.de/thorsten/tnt. W. Daelemans and H. Strik. 2002. Het Nederlands in Taal- en Spraaktechnologie: Prioriteiten voor Basisvoorzieningen. Technical report, Nederlandse Taalunie. B. Dewulf and G. Saerens. 2000. Stijlboek Teletekst Ondertiteling. Technical report, VRT, Brussel. Internal Subtitling Guidelines. W. Haeseryn, G. Geerts, J de Rooij, and M. van den Toorn. 1997. Algemene Nederlandse Spraakkunst. Martinus Nijhoff Uitgevers, Groningen. ITC. 1997. Guidance on standards for subtitling. Technical report, ITC. Online at http://www.itc.org.uk/ codes guidelines/broadcasting/tv/sub sign audio/subtitling stnds/. H. Jing. 2001. Cut-and-Paste Text Summarization. Ph.D. thesis, Columbia University. F.J. Koopmans-van Beinum and M.E. van Donzel. 1996. Relationship Between Discourse Structure and Dynamic Speech Rate. In Proceedings ICSLP 1996, Philadelphia, USA. N. Oostdijk, W. Goedertier, F. Van Eynde, L. Boves, J.P. Marters, M. Moortgat, and H. Baayen. 2002. Experiences from the Spoken Dutch Corpus. In Proceedings of LREC 2002, volume I, pages 340– 347, Paris. ELRA. F. Van Eynde. 2004. Part-of-speech Tagging en Lemmatisering. Internal manual of Corpus Gesproken Nederlands, published online at http://www.ccl.kuleuven.ac.be/Papers/ POSmanual febr2004.pdf. V. Vandeghinste and E. Tjong Kim Sang. 2004. Using a parallel transcript/subtitle corpus for sentence compression. In Proceedings of LREC 2004, Paris. ELRA. V. Vandeghinste. 2002. Lexicon optimization: Maximizing lexical coverage in speech recognition through automated compounding. In Proceedings of LREC 2002, volume IV, pages 1270– 1276, Paris. ELRA. V. Vandeghinste. submitted. ShaRPa: Shallow Rule-based Parsing, focused on Dutch. In Proceedings of CLIN 2003.</biblio>
  </article>
  <article>
    <preamble>marcu_statistics_sentence_pass_one.txt</preamble>
    <titre>From: AAAI-00 Proceedings. Copyright © 2000, AAAI (www.aaai.org). All rights reserved. Statistics-Based Summarization — Step One: Sentence Compression Kevin Knight and Daniel Marcu</titre>
    <auteur>Information Sciences Institute and Department of Computer Science University of Southern California 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292 {knight,marcu}@isi.edu</auteur>
    <abstract>When humans produce summaries of documents, they do not simply extract sentences and concatenate them. Rather, they create new sentences that are grammatical, that cohere with one another, and that capture the pairs are available online, it is now possible to envision algorithms that are trained to mimic this process. In this paper, we focus on sentence compression, a simpler version of this larger challenge. We aim to achieve two goals simultaneously: our compressions should be grammatical, and they should retain the most important pieces of information. These two goals can conﬂict. We devise both noisy-channel and decision-tree approaches to the problem, and we evaluate results against manual compressions and a simple baseline. grammatical sentences. All other approaches employ sets of manually written or semi-automatically derived c 2000, American Association for Artiﬁcial InCopyright  telligence (www.aaai.org). All rights reserved. rules for deleting information that is redundant, compressing long sentences into shorter ones, aggregating sentences, repairing reference links, etc. available, in order to automatically learn how to rewrite in the statistical MT community, which is focused on sentence-to-sentence translations, we also decided to focus ﬁrst on a simpler problem, that of sentence compression. We chose this problem for two reasons: • First, the problem is complex enough to require the development of sophisticated compression models: Determining what is important in a sentence and determining how to convey the important information grammatically, using only a few words, is just a scaled down version of the text summarization problem. Yet, the problem is simple enough, since we do not have to worry yet about discourse related issues, such as coherence, anaphors, etc. • Second, an adequate solution to this problem has an immediate impact on several applications. For example, due to time and space constraints, the generation of TV captions often requires only the most important parts of sentences to be shown on a screen (Linke-Ellis 1999; Robert-Ribes et al. 1999). A good sentence compression module would therefore have an impact on the task of automatic caption generation. A sentence compression module sentence compression rules (McKeown et al. 1999; Mani, Gates, & Bloedorn 1999; Barzilay, McKeown, & Elhadad 1999), it is likely that a good sentence compression module would impact the overall quality of these systems as well. This becomes particularly important for text genres that use long sentences. In this paper, we present two approaches to the sentence compression problem. Both take as input a sequence of words W = w1 , w2 , . . . , wn (one sentence). An algorithm may drop any subset of these words. The words that remain (order unchanged) form a compression. There are 2n compressions to choose from—some are reasonable, most are not. Our ﬁrst approach develops a probabilistic noisy-channel model for sentence compression. The second approach develops a decisionbased, deterministic model. A noisy-channel model for sentence compression This section describes a probabilistic approach to the compression problem. In particular, we adopt the noisy channel framework that has been relatively successful in a number of other NLP applications, including speech recognition (Jelinek 1997), machine translation (Brown et al. 1993), part-of-speech tagging (Church 1988), transliteration (Knight & Graehl 1998), and information retrieval (Berger & Laﬀerty 1999). In this framework, we look at a long string and imagine that (1) it was originally a short string, and then (2) someone added some additional, optional text to it. Compression is a matter of identifying the original short string. It is not critical whether or not the “original” string is real or hypothetical. For example, in statistical machine translation, we look at a French string and say, “This was originally English, but someone added ‘noise’ to it.” The French may or may not have been translated from English originally, but by removing the noise, we can hypothesize an English source—and thereby translate the string. In the case of compression, the noise consists of optional text material that pads out the core signal. For the larger case of text summarization, it may be useful to imagine a scenario in which a news editor composes a short document, hands it to a reporter, and tells the reporter to “ﬂesh it out” . . . which results in the article we read in the newspaper. As summarizers, we may not have access to the editor’s original version (which may or may not exist), but we can guess at it— which is where probabilities come in. As in any noisy channel application, we must solve three problems: • Source model. We must assign to every string s a probability P(s), which gives the chance that s is generated as an “original short string” in the above hypothetical process. For example, we may want P(s) to be very low if s is ungrammatical. • Channel model. We assign to every pair of strings s, t a probability P(t | s), which gives the chance that when the short string s is expanded, the result is the long string t. For example, if t is the same as s except for the extra word “not,” then we may want P(t | s) to be very low. The word “not” is not optional, additional material. • Decoder. When we observe a long string t, we search for the short string s that maximizes P(s | t). This is equivalent to searching for the s that maximizes P(s) · P (t | s). It is advantageous to break the problem down this way, as it decouples the somewhat independent goals of creating a short text that (1) looks grammatical, and (2) preserves important information. It is easier to build a channel model that focuses exclusively on the latter, without having to worry about the former. That is, we can specify that a certain substring may represent unimportant information, but we do not need to worry that deleting it will result in an ungrammatical structure. We leave that to the source model, which worries exclusively about well-formedness. In fact, we can make use of extensive prior work in source language modeling for speech recognition, machine translation, and natural language generation. The same goes for actual compression (“decoding” in noisy-channel jargon)—we can re-use generic software packages to solve problems in all these application domains. Statistical Models In the experiments we report here, we build very simple source and channel models. In a departure from the above discussion and from previous work on statistical channel models, we assign probabilities Ptree (s) and Pexpand tree (t | s) to trees rather than strings. In decoding a new string, we ﬁrst parse it into a large tree t (using Collins’ parser (1997)), and we then hypothesize and rank various small trees. Good source strings are ones that have both (1) a normal-looking parse tree, and (2) normal-looking word pairs. Ptree (s) is a combination of a standard probabilistic context-free grammar (PCFG) score, which is computed over the grammar rules that yielded the tree s, and a standard word-bigram score, which is computed over the leaves of the tree. For example, the tree s =(S (NP John) (VP (VB saw) (NP Mary))) is assigned a score based on these factors: Ptree (s) = P(TOP → S | TOP) · P(S → NP VP | S) · P(NP → John | NP) · P(VP → VB NP | VP) · P(VP → saw | VB) · P(NP → Mary | NP) · P(John | EOS) · P(saw | John) · P(Mary | saw) · P(EOS | Mary) Our stochastic channel model performs minimal operations on a small tree s to create a larger tree t. For each internal node in s, we probabilistically choose an expansion template based on the labels of the node and its children. For example, when processing the S node in the tree above, we may wish to add a prepositional phrase as a third child. We do this with probability P(S → NP VP PP | S → NP VP). Or we may choose to leave it alone, with probability P(S → NP VP | S → NP VP). After we choose an expansion template, then for each new child node introduced (if any), we grow a new subtree rooted at that node—for example (PP (P in) (NP Pittsburgh)). Any particular subtree is grown with probability given by its PCFG factorization, as above (no bigrams). G G A H a C G A H D B b Q R Z d a e D F C D b e H K a b e c (t) Figure 1: Examples of parse trees. Although the modules themselves may be physically and/or electrically incompatible, the cable-speciﬁc jacks on them provide industry-standard connections. Cable-speciﬁc jacks provide industry-standard connections. Example In this section, we show how to tell whether one potential compression is more likely than another, according to the statistical models described above. Suppose we observe the tree t in Figure 1, which spans the string abcde. Consider the compression s1, which is shown in the same ﬁgure. We compute the factors Ptree (s1) and Pexpand tree (t | s1). Breaking this down further, the source PCFG and word-bigram factors, which describe Ptree (s1), are: P(TOP → G | TOP) P(G → H A | G) P(A → C D | A) P(H → a | H) P(C → b | C) P(D → e | D) P(a | EOS) P(b | a) P(e | b) P(EOS | e) The channel expansion-template factors and the channel PCFG (new tree growth) factors, which describe Pexpand tree (t | s1), are: P(G → H A | G → H A) P(A → C B D | A → C D) P(B → Q R | B) P(Q → Z | Q) All of our design goals were achieved and the delivered performance matches the speed of the underlying device. All design goals were achieved. Reach’s E-mail product, MailMan, is a message- management system designed initially for VINES LANs that will eventually be operating system-independent. MailMan will eventually be operating system-independent. (s2) (s1) The documentation is typical of Epson quality: excellent. Documentation is excellent. P(Z → c | Z) P(R → d | R) A diﬀerent compression will be scored with a diﬀerent set of factors. For example, consider a compression of t that leaves t completely untouched. In that case, the source costs Ptree (t) are: P(TOP → G | TOP) P(G → H A | G) P(H → a | H) P(C → b | C) P(a | EOS) P(b | a) P(A → C D | A) P(Z → c | Z) P(c | b) P(B → Q R | B) P(Q → Z | Q) P(R → d | R) P(D → e | D) P(d | c) P(e | d) P(EOS | e) The channel costs Pexpand tree (t | t) are: Ingres/Star prices start at $2,100. Ingres/Star prices start at $2,100. Figure 2: Examples from our parallel corpus. P(G → H A | G → H A) P(A → C B D | A → C B D) P(B → Q R | B → Q R) P(Q → Z | Q → Z) Now we can simply compare Pexpand tree (s1 | t) = Ptree (s1) · Pexpand tree (t | s1))/Ptree (t) versus Pexpand tree (t | t) = Ptree (t) · Pexpand tree (t | t))/Ptree (t) and select the more likely one. Note that Ptree (t) and all the PCFG factors can be canceled out, as they appear in any potential compression. Therefore, we need only compare compressions of the basis of the expansion-template probabilities and the word-bigram probabilities. The quantities that diﬀer between the two proposed compressions are boxed above. Therefore, s1 will be preferred over t if and only if: P(e | b) · P(A → C B D | A → C D) > P(b | a) · P(c | b) · P(d | c) · P(A → C B D | A → C B D) · P(B → Q R | B → Q R) · P(Q → Z | Q → Z) Training Corpus In order to train our system, we used the Ziﬀ-Davis corpus, a collection of newspaper articles announcing computer products. Many of the articles in the corpus pairs. Each pair consisted of a sentence t = t1 , t2 , . . . , tn that occurred in the article and a possibly compressed version of it s = s1 , s2 , . . . , sm , which occurred in the pairs extracted from the corpus. We decided to use such a corpus because it is consistent with two desiderata speciﬁc to summarization compressed form the salient points of the original newspaper Sentences. We decided to keep in the corpus uncompressed sentences as well, since we want to learn not only how to compress a sentence, but also when to do it. Learning Model Parameters We collect expansion-template probabilities from our parallel corpus. We ﬁrst parse both sides of the parallel corpus, and then we identify corresponding syntactic nodes. For example, the parse tree for one sentence may begin (S (NP . . . ) (VP . . . ) (PP . . . )) while the parse tree for its compressed version may begin (S (NP . . . ) (VP . . . )). If these two S nodes are deemed to correspond, then we chalk up one joint event (S → NP VP, S → NP VP PP); afterwards we normalize. Not all nodes have corresponding partners; some noncorrespondences are due to incorrect parses, while others are due to legitimate reformulations that are beyond the scope of our simple channel model. We use standard methods to estimate word-bigram probabilities. Beyond that basic level, the operations of the three products vary widely (1514588) Beyond that level, the operations of the three products vary widely (1430374) Beyond that basic level, the operations of the three products vary (1333437) Beyond that level, the operations of the three products vary (1249223) Beyond that basic level, the operations of the products vary (1181377) Decoding The operations of the three products vary widely (939912) There is a vast number of potential compressions of a large tree t, but we can pack them all eﬃciently into a shared-forest structure. For each node of t that has n children, we • generate 2n − 1 new nodes, one for each non-empty subset of the children, and • pack those nodes so that they are referred to as a whole. For example, consider the large tree t above. All compressions can be represented with the following forest: G→HA G→H G→A B→QR B→Q semantic representation into a vast number of potential English renderings. These renderings are packed into a forest, from which the most promising sentences are extracted using statistical scoring. For our purposes, the extractor selects the trees with the best combination of word-bigram and expansiontemplate scores. It returns a list of such trees, one for each possible compression length. For example, for the sentence Beyond that basic level, the operations of the three products vary, we obtain the following “best” compressions, with negative log-probabilities shown in parentheses (smaller = more likely): B→R Q→Z A→CBD A→CB A→CD A→BC A→C A→B A→D H→a C→b Z→c R→d D→e We can also assign an expansion-template probability to each node in the forest. For example, to the B → Q node, we can assign P(B → Q R | B → Q). If the observed probability from the parallel corpus is zero, then we assign a small ﬂoor value of 10−6 . In reality, we produce forests that are much slimmer, as we only consider compressing a node in ways that are locally grammatical according to the Penn Treebank—if a rule of the type A → C B has never been observed, then it will not appear in the forest. At this point, we want to extract a set of highscoring trees from the forest, taking into account both expansion-template probabilities and word-bigram probabilities. Fortunately, we have such a generic extractor on hand (Langkilde 2000). This extractor was The operations of the products vary widely (872066) The operations of the products vary (748761) The operations of products vary (690915) Operations of products vary (809158) The operations vary (522402) Operations vary (662642) Length Selection It is useful to have multiple answers to choose from, as one user may seek a 20% compression, while another seeks a 60% compression. However, for purposes of evaluation, we want our system to be able to select a single compression. If we rely on the log-probabilities as shown above, we will almost always choose the shortest compression. (Note above, however, how the threeword compression scores better than the two-word compression, as the models are not entirely happy removing the article “the”). To create a more fair competition, we divide the log-probability by the length of the compression, rewarding longer strings. This is commonly done in speech recognition. If we plot this normalized score against compression length, we usually observe a (bumpy) U-shaped curve, as illustrated in Figure 3. In a typical more diﬃcult case, a 25-word sentence may be optimally compressed by a 17-word version. Of course, if a user requires a shorter compression than that, she may select another region of the curve and look for a local minimum. A decision-based model for sentence compression In this section, we describe a decision-based, history model of sentence compression. As in the noisy-channel approach, we again assume that we are given as input 0.10 4 5 6 7 8 Finally, another advantage of broadband is distance . Finally another advantage of broadband is distance . Another advantage of broadband is distance . Advantage of broadband is distance . 0.15 Another advantage is distance . Advantage is distance . Adjusted negative log-probability of best compression s at a particular length n -log P(s) P( t | s) / n 0.20 Stack 9 Compression length n Figure 3: Adjusted log-probabilities for top-scoring compressions at various lengths (lower is better). a parse tree t. Our goal is to “rewrite” t into a smaller tree s, which corresponds to a compressed version of the original sentence subsumed by t. Suppose we observe in our corpus the trees t and s2 in Figure 1. In this model, we ask ourselves how we may go about rewriting t into s2. One possible solution is to decompose the rewriting operation into a sequence of shift-reduce-drop actions that are speciﬁc to an extended shift-reduce parsing paradigm. In the model we propose, the rewriting process starts with an empty Stack and an Input List that contains the sequence of words subsumed by the large tree t. Each word in the input list is labeled with the name of all syntactic constituents in t that start with it (see Figure 4). At each step, the rewriting module applies an operation that is aimed at reconstructing the smaller tree s2. In the context of our sentence-compression module, we need four types of operations: • shift operations transfer the ﬁrst word from the input list into the stack; • reduce operations pop the k syntactic trees located at the top of the stack; combine them into a new tree; and push the new tree on the top of the stack. Reduce operations are used to derive the structure of the syntactic tree of the short sentence. • drop operations are used to delete from the input list subsequences of words that correspond to syntactic constituents. A drop x operations deletes from the Input List Input List Stack F G H a H A a C b H K a b B Q Z c A B Q Z c C b B Q Z c R d R d R d D e SHIFT; K a b ASSIGNTYPE H F STEPS 1-2 D e SHIFT; H K a b F R d D e D e DROP B STEP 6 SHIFT; ASSIGNTYPE D STEPS 7-8 D ASSIGNTYPE K STEPS 3-4 D e H B Q Z c H K a b e REDUCE 2 G STEP 9 G REDUCE 2 F STEP 5 F D H K a b e Figure 4: Example of incremental tree compression. input list all words that are spanned by constituent x in t. • assignType operations are used to change the label of trees at the top of the stack. These actions assign POS tags to the words in the compressed sentence, which may be diﬀerent from the POS tags in the original sentence. The decision-based model is more ﬂexible than the channel model because it enables the derivation of trees whose skeleton can diﬀer quite drastically from that of the tree given as input. For example, using the channel model, we are unable to obtain tree s2 from t. However, the four operations listed above enable us to rewrite a tree t into any tree s, as long as an in-order traversal of the leaves of s produces a sequence of words that occur in the same order as the words in the tree t. For example, the tree s2 can be obtained from tree t by following this sequence of actions, whose eﬀects are shown in Figure 4: shift; assignType H; shift; assignType K; reduce 2 F; drop B; shift; assignType D; reduce 2 G. To save space, we show shift and assignType operations on the same line; however, the reader should understand that they correspond to two distinct actions. As one can see, the assignType K operation rewrites the POS tag of the word b; the reduce operations modify the skeleton of the tree given as input. To increase readability, the input list is shown in a format that resembles as closely as possible the graphical representation of the trees in ﬁgure 1. Learning the parameters of the decision-based model We associate with each conﬁguration of our shiftreduce-drop, rewriting model a learning case. The cases are generated automatically by a program that derives sequences of actions that map each of the large trees in our corpus into smaller trees. The rewriting procedure simulates a bottom-up reconstruction of the smaller trees. Overall, the 1067 pairs of long and short sentences yielded 46383 learning cases. Each case was labeled with one action name from a set of 210 possible actions: There are 37 distinct assignType actions, one for each POS tag. There are 63 distinct drop actions, one for each type of syntactic constituent that can be deleted during compression. There are 109 distinct reduce actions, one for each type of reduce operation that is applied during the reconstruction of the compressed sentence. And there is one shift operation. Given a tree t and an arbitrary conﬁguration of the stack and input list, the purpose of the decision-based classiﬁer is to learn what action to choose from the set of 210 possible actions. To each learning example, we associated a set of 99 features from the following two classes: Operational features reﬂect the number of trees in the stack, the input list, and the types of the last ﬁve operations. They also encode information that denote the syntactic category of the root nodes of the partial trees built up to a certain time. Examples of such features are: numberTreesInStack, wasPreviousOperationShift, syntacticLabelOfTreeAtTheTopOfStack, etc. Original-tree-speciﬁc features denote the syntactic constituents that start with the ﬁrst unit in the input list. Examples of such features are: inputListStartsWithA CC, inputListStartsWithA PP, etc. The decision-based compression module uses the C4.5 program (Quinlan 1993) in order to learn decision trees that specify how large syntactic trees can be compressed into shorter trees. A ten-fold crossvalidation evaluation of the classiﬁer yielded an accuracy of 87.16% (± 0.14). A majority baseline classiﬁer that chooses the action shift has an accuracy of 28.72%. Employing the decision-based model To compress sentences, we apply the shift-reduce-drop model in a deterministic fashion. We parse the sentence to be compressed (Collins 1997) and we initialize the input list with the words in the sentence and the syntactic constituents that “begin” at each word, as shown in Figure 4. We then incrementally inquire the learned classiﬁer what action to perform, and we simulate the execution of that action. The procedure ends when the input list is empty and when the stack contains only one tree. An inorder traversal of the leaves of this tree produces the compressed version of the sentence given as input. Since the model is deterministic, it produces only one output. The advantage is that the compression is very fast: it takes only a few milliseconds per sentence. The disadvantage is that it does not produce a range of compressions, from which another system may subsequently choose. It is straightforward though to extend the model within a probabilistic framework by applying, for example, the techniques used by Magerman (1995). Evaluation To evaluate our compression algorithms, we randomly selected 32 sentence pairs from our parallel corpus, which we will refer to as the Test Corpus. We used the other 1035 sentence pairs for training. Figure 5 shows three sentences from the Test Corpus, together with the compressions produced by humans, our compression algorithms, and a baseline algorithm that produces compressions with highest word-bigram scores. The examples are chosen so as to reﬂect good, average, and bad performance cases. The ﬁrst sentence is compressed in the same manner by humans and our algorithms (the baseline algorithm chooses though not to compress this sentence). For the second example, the output of the Decision-based algorithm is grammatical, but the semantics is negatively aﬀected. The noisy-channel algorithm deletes only the word “break”, which aﬀects the correctness of the output less. In the last example, the noisy-channel model is again more conservative and decides not to drop any constituents. In constrast, the decision-based algorithm compresses the input substantially, but it fails to produce a grammatical output. We presented each original sentence in the Test Corpus to four judges, together with four compressions of it: the human generated compression, the outputs of the noisy-channel and decision-based algorithms, and the output of the baseline algorithm. The judges were told that all outputs were generated automatically. The order of the outputs was scrambled randomly across test cases. To avoid confounding, the judges participated in two experiments. In the ﬁrst experiment, they were asked to determine on a scale from 1 to 5 how well the systems did with respect to selecting the most important words in the original sentence. In the second experiment, they were asked to determine on a scale from 1 to 5 how grammatical the outputs were. We also investigated how sensitive our algorithms are with respect to the training data by carrying out the same experiments on sentences of a diﬀerent genre, the scientiﬁc one. To this end, we took the ﬁrst sentence of the ﬁrst 26 articles made available in 1999 on the cmplg archive. We created a second parallel corpus, which we will refer to as the Cmplg Corpus, by generating by ourselves compressed grammatical versions of these sentences. Since some of the sentences in this corpus were extremely long, the baseline algorithm could not produce compressed versions in reasonable time. The results in Table 1 show compression rates, and mean and standard deviation results across all judges, for each algorithm and corpus. The results show that the decision-based algorithm is the most aggressive: on average, it compresses sentences to about half of their original size. The compressed sentences produced by both algorithms are more “grammatical” and contain more important words than the sentences produced by the baseline. T -test experiments showed these diﬀerences to be statistically signiﬁcant at p < 0.01 both for individual judges and for average scores across Original: Beyond the basic level, the operations of the three products vary widely. Baseline: Beyond the basic level, the operations of the three products vary widely. Noisy-channel: The operations of the three products vary widely. Decision-based: The operations of the three products vary widely. Humans: The operations of the three products vary widely. Original: Arborscan is reliable and worked accurately in testing, but it produces very large dxf ﬁles. Baseline: Arborscan and worked in, but it very large dxf. Noisy-channel: Arborscan is reliable and worked accurately in testing, but it produces very large dxf ﬁles. Decision-based: Arborscan is reliable and worked accurately in testing very large dxf ﬁles. Humans: Arborscan produces very large dxf ﬁles. Original: Many debugging features, including user-deﬁned break points and variable-watching and message-watching windows, have been added. Baseline: Debugging, user-deﬁned and variable-watching and message-watching, have been. Noisy-channel: Many debugging features, including user-deﬁned points and variable-watching and message-watching windows, have been added. Decision-based: Many debugging features. Humans: Many debugging features have been added . Figure 5: Compression examples Corpus Test Avg. orig. sent. length 21 words Cmplg 26 words Compression Grammaticality Importance Compression Grammaticality Importance Baseline 63.70%</abstract>
    <biblio>Barzilay, R.; McKeown, K.; and Elhadad, M. 1999. Information fusion in the context of multi-document summarization. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL–99), 550–557. Berger, A., and Laﬀerty, J. 1999. Information retrieval as statistical translation. In Proceedings of the 22nd Conference on Research and Development in Information Retrieval (SIGIR–99), 222–229. Brown, P.; Della Pietra, S.; Della Pietra, V.; and Mercer, R. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics 19(2):263–311. Church, K. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Proceedings of the Second Conference on Applied Natural Language Processing, 136–143. Collins, M. 1997. Three generative, lexicalized models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL–97), 16–23. Grefenstette, G. 1998. Producing intelligent telegraphic text reduction to provide an audio scanning service for the blind. In Working Notes of the AAAI Spring Symposium on Intelligent Text Summarization, 111–118. Jelinek, F. 1997. Statistical Methods for Speech Recognition. The MIT Press. Jing, H., and McKeown, K. 1999. The decomposition of human-written summary sentences. In Proceedings of the 22nd Conference on Research and Development in Information Retrieval (SIGIR–99). Knight, K., and Graehl, J. 1998. Machine transliteration. Computational Linguistics 24(4):599–612. Langkilde, I. 2000. Forest-based statistical sentence generation. In Proceedings of the 1st Annual Meeting of the North American Chapter of the Association for Computational Linguistics. Linke-Ellis, N. 1999. Closed captioning in America: Looking beyond compliance. In Proceedings of the TAO Workshop on TV Closed Captions for the hearing impaired people, 43–59. Magerman, D. 1995. Statistical decision-tree models for parsing. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, 276– 283. Mani, I., and Maybury, M., eds. 1999. Advances in Automatic Text Summarization. The MIT Press. Mani, I.; Gates, B.; and Bloedorn, E. 1999. Improving summaries by revising them. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, 558–565. McKeown, K.; Klavans, J.; Hatzivassiloglou, V.; Barzilay, R.; and Eskin, E. 1999. Towards multidocument summarization by reformulation: Progress and prospects. In Proceedings of the Sixteenth National Conference on Artiﬁcial Intelligence (AAAI–99). Quinlan, J. 1993. C4.5: Programs for Machine Learning. San Mateo, CA: Morgan Kaufmann Publishers. Robert-Ribes, J.; Pfeiﬀer, S.; Ellison, R.; and Burnham, D. 1999. Semi-automatic captioning of TV programs, an Australian perspective. In Proceedings of the TAO Workshop on TV Closed Captions for the hearing impaired people, 87–100. Witbrock, M., and Mittal, V. 1999. Ultrasummarization: A statistical approach to generating highly condensed non-extractive summaries. In Proceedings of the 22nd International Conference on Research and Development in Information Retrieval (SIGIR’99), Poster Session, 315–316.</biblio>
  </article>
  <article>
    <preamble>mikheev.txt</preamble>
    <titre>Periods, Capitalized Words, etc. Andrei Mikheev∗</titre>
    <auteur>University of Edinburgh In this article we present an approach for tackling three important aspects of text normalization: sentence boundary disambiguation, disambiguation of capitalized words in positions where capitalization is expected, and identification of abbreviations. As opposed to the two dominant techniques of computing statistics or writing specialized grammars, our document-centered approach works by considering suggestive local contexts and repetitions of individual words within a document. This approach proved to be robust to domain shifts and new lexica and produced performance on the level with the highest reported results. When incorporated into a part-of-speech tagger, it helped reduce the error rate significantly on capitalized words and sentence boundaries. We also investigated the portability to other languages and obtained encouraging results.</auteur>
    <abstract></abstract>
    <biblio>Aberdeen, John S., John D. Burger, David S. Day, Lynette Hirschman, Patricia Robinson, and Marc Vilain. 1995. “Mitre: Description of the alembic system used for MUC-6.” In Proceedings of the Sixth Message Understanding Conference (MUC-6), Columbia, Maryland, November. Morgan Kaufmann. Baldwin, Breck, Christine Doran, Jeffrey Reynar, Michael Niv, Bangalore Srinivas, and Mark Wasson. 1997. “EAGLE: An extensible architecture for general linguistic engineering.” In Proceedings of Computer-Assisted Information Searching on 316 Internet (RIAO ’97), Montreal, June. Baum, Leonard E. and Ted Petrie. 1966. Statistical inference for probabilistic functions of finite Markov chains. Annals of Mathematical Statistics 37:1559–1563. Bikel, Daniel, Scott Miller, Richard Schwartz, and Ralph Weischedel. 1997. “Nymble: A high performance learning name-finder.” In Proceedings of the Fifth Conference on Applied Natural Language Processing (ANLP’97), pages 194–200. Washington, D.C., Morgan Kaufmann. Brill, Eric. 1995a. Transformation-based error-driven learning and natural language parsing: A case study in part-of-speech tagging. Computational Linguistics 21(4):543–565. Brill, Eric. 1995b. “Unsupervised learning of disambiguation rules for part of speech tagging.” In David Yarovsky and Kenneth Church, editors, Proceedings of the Third Workshop on Very Large Corpora, pages 1–13, Somerset, New Jersey. Association for Computational Linguistics. Burnage, Gavin. 1990. CELEX: A Guide for Users. Centre for Lexical Information, Nijmegen, Netherlands. Mikheev Chinchor, Nancy. 1998. “Overview of MUC-7.” In Seventh Message Understanding Conference (MUC-7): Proceedings of a Conference Held in Fairfax, April. Morgan Kaufmann. Church, Kenneth. 1988. “A stochastic parts program and noun-phrase parser for unrestricted text.” In Proceedings of the Second ACL Conference on Applied Natural Language Processing (ANLP’88), pages 136–143, Austin, Texas. Church, Kenneth. 1995. “One term or two?” In SIGIR’95, Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 310–318, Seattle, Washington, July. ACM Press. Clarkson, Philip and Anthony J. Robinson. 1997. “Language model adaptation using mixtures and an exponentially decaying cache.” In Proceedings IEEE International Conference on Speech and Signal Processing, Munich, Germany. Cucerzan, Silviu and David Yarowsky. 1999. “Language independent named entity recognition combining morphological and contextual evidence.” In Proceedings of Joint SIGDAT Conference on EMNLP and VLC. Francis, W. Nelson and Henry Kucera. 1982. Frequency Analysis of English Usage: Lexicon and Grammar. Houghton Mifflin, New York. Gale, William, Kenneth Church, and David Yarowsky. 1992. “One sense per discourse.” In Proceedings of the Fourth DARPA Speech and Natural Language Workshop, pages 233–237. Grefenstette, Gregory and Pasi Tapanainen. 1994. “What is a word, what is a sentence? Problems of tokenization.” In The Proceedings of Third Conference on Computational Lexicography and Text Research (COMPLEX’94), Budapest, Hungary. Krupka, George R. and Kevin Hausman. 1998. Isoquest Inc.: Description of the netowl extractor system as used for MUC-7. In Proceedings of the Seventh Message Understanding Conference (MUC-7), Fairfax, VA. Morgan Kaufmann. Kuhn, Roland and Renato de Mori. 1998. A cache-based natural language model for speech recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence 12:570–583. Kupiec, Julian. 1992. Robust part-of-speech tagging using a hidden Markov model. Computer Speech and Language. Mani, Inderjeet and T. Richard MacMillan. 1995. “Identifying unknown proper Periods, Capitalized Words, etc. names in newswire text.” In B. Boguraev and J. Pustejovsky, editors, Corpus Processing for Lexical Acquisition. MIT Press, Cambridge, Massachusetts, pages 41–59. Marcus, Mitchell, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: The Penn treebank. Computational Linguistics 19(2):313–329. Mikheev, Andrei. 1997. Automatic rule induction for unknown word guessing. Computational Linguistics 23(3):405–423. Mikheev, Andrei. 1999. A knowledge-free method for capitalized word disambiguation. In Proceedings of the 37th Conference of the Association for Computational Linguistics (ACL’99), pages 159–168, University of Maryland, College Park. Mikheev, Andrei. 2000. “Tagging sentence boundaries.” In Proceedings of the First Meeting of the North American Chapter of the Computational Linguistics (NAACL’2000), pages 264–271, Seattle, Washington. Morgan Kaufmann. Mikheev, Andrei, Clair Grover, and Colin Matheson. 1998. TTT: Text Tokenisation Tool. Language Technology Group, University of Edinburgh. Available at http://www.ltg.ed.ac.uk/software/ttt/ index.html. Mikheev, Andrei, Clair Grover, and Marc Moens. 1998. Description of the ltg system used for MUC-7. In Seventh Message Understanding Conference (MUC–7): Proceedings of a Conference Held in Fairfax, Virginia. Morgan Kaufmann. Mikheev, Andrei and Liubov Liubushkina. 1995. Russian morphology: An engineering approach. Natural Language Engineering 1(3):235–260. Palmer, David D. and Marti A. Hearst. 1994. “Adaptive sentence boundary disambiguation.” In Proceedings of the Fourth ACL Conference on Applied Natural Language Processing (ANLP’94), pages 78–83, Stuttgart, Germany, October. Morgan Kaufmann. Palmer, David D. and Marti A. Hearst. 1997. Adaptive multilingual sentence boundary disambiguation. Computational Linguistics 23(2):241–269. Park, Youngja and Roy J. Byrd. 2001. “Hybrid text mining for finding abbreviations and their definitions.” In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMLP’01), pages 16–19, Washington, D.C. Morgan Kaufmann. Ratnaparkhi, Adwait. 1996. “A maximum entropy model for part-of-speech 317 Computational Linguistics tagging.” In Proceedings of Conference on Empirical Methods in Natural Language Processing, pages 133–142, University of Pennsylvania, Philadelphia. Reynar, Jeffrey C. and Adwait Ratnaparkhi. 1997. “A maximum entropy approach to identifying sentence boundaries.” In Proceedings of the Fifth ACL Conference on Applied Natural Language Processing (ANLP’97), pages 16–19. Morgan Kaufmann. Riley, Michael D. 1989. “Some applications of tree-based modeling to speech and 318 Volume 28, Number 3 language indexing.” In Proceedings of the DARPA Speech and Natural Language Workshop, pages 339–352. Morgan Kaufmann. Yarowsky, David. 1993. “One sense per collocation.” In Proceedings of ARPA Human Language Technology Workshop ’93, pages 266–271, Princeton, New Jersey. Yarowsky, David. 1995. “Unsupervised word sense disambiguation rivaling supervised methods.” In Meeting of the Association for Computational Linguistics (ACL’95), pages 189–196.</biblio>
  </article>
  <article>
    <preamble>probabilistic_sentence_reduction.txt</preamble>
    <titre>Probabilistic Sentence Reduction Using Support Vector Machines Minh Le Nguyen, Akira Shimazu, Susumu Horiguchi Bao Tu Ho and Masaru Fukushi</titre>
    <auteur>Japan Advanced Institute of Science and Technology 1-8, Tatsunokuchi, Ishikawa, 923-1211, JAPAN {nguyenml, shimazu, hori, bao, mfukushi}@jaist.ac.jp</auteur>
    <abstract>This paper investigates a novel application of support vector machines (SVMs) for sentence reduction. We also propose a new probabilistic sentence reduction method based on support vector machine learning. Experimental results show that the proposed methods outperform earlier methods in term of sentence reduction performance. 1</abstract>
    <biblio>A. Borthwick, “A Maximum Entropy Approach to Named Entity Recognition”, Ph.D thesis, Computer Science Department, New York University (1999). C.-C. Chang and C.-J. Lin, “LIBSVM: a library for support vector machines”, Software available at http://www.csie.ntu.edu.tw/ cjlin/libsvm. H. Jing, “Sentence reduction for automatic text summarization”, In Proceedings of the First Annual Meeting of the North American Chapter of the Association for Computational Linguistics NAACL-2000. T.T. Hastie and R. Tibshirani, “Classification by pairwise coupling”, The Annals of Statistics, 26(1): pp. 451-471, 1998. C.-W. Hsu and C.-J. Lin, “A comparison of methods for multi-class support vector machines”, IEEE Transactions on Neural Networks, 13, pp. 415-425, 2002. K. Knight and D. Marcu, “Summarization beyond sentence extraction: A Probabilistic approach to sentence compression”, Artificial Intelligence 139: pp. 91-107, 2002. C.Y. Lin, “Improving Summarization Performance by Sentence Compression — A Pilot Study”, Proceedings of the Sixth International Workshop on Information Retrieval with Asian Languages, pp.1-8, 2003. C. Macleod and R. Grishman, “COMMLEX syntax Reference Manual”; Proteus Project, New York University (1995). M.L. Nguyen and S. Horiguchi, “A new sentence reduction based on Decision tree model”, Proceedings of 17th Pacific Asia Conference on Language, Information and Computation, pp. 290-297, 2003 V. Vapnik, “The Natural of Statistical Learning Theory”, New York: Springer-Verlag, 1995. J. Platt,“ Probabilistic outputs for support vector machines and comparison to regularized likelihood methods,” in Advances in Large Margin Classifiers, Cambridege, MA: MIT Press, 2000. B. Scholkopf et al, “Comparing Support Vector Machines with Gausian Kernels to Radius Basis Function Classifers”, IEEE Trans. Signal Procesing, 45, pp. 2758-2765, 1997.</biblio>
  </article>
  <article>
    <preamble>Stolcke_1996_Automatic_linguistic.txt</preamble>
    <titre>AUTOMATIC LINGUISTIC SEGMENTATION OF CONVERSATIONAL SPEECH Andreas Stolcke Elizabeth Shriberg Speech Technology and Research Laboratory SRI International, Menlo Park, CA 94025</titre>
    <auteur>stolcke@speech.sri.com ees@speech.sri.com</auteur>
    <abstract>As speech recognition moves toward more unconstrained domains such as conversational speech, we encounter a need to be able to segment (or resegment) waveforms and recognizer output into linguistically meaningful units, such a sentences. Toward this end, we present a simple automatic segmenter of transcripts based on N-gram language modeling. We also study the relevance of several word-level features for segmentation performance. Using only word-level information, we achieve 85% recall and 70% precision on linguistic boundary detection. words, the tag set could be optimized to provide the right level of resolution for the segmentation task. It should be noted that the results for POS-based models are optimistic in the sense that for an actual application one would first have to tag the input with POS labels, and then apply the segmentation model. The actual performance would be degraded by tagging errors. 5.4. Error Trade-offs As an aside to our search for useful features for the segmentation task, we observe that we can optimize any particular language model by trading off recall performance for false alarm rate, or vice versa. We did this by biasing the likelihoods of S states by some constant factor, causing the Viterbi algorithm to choose these states more often. Table 4 compares two bias values, and shows that the bias can be used to increase both recall and precision, while also reducing the segment error rate. Using Part-of-Speech Information So far we have used only the identity of words. It is likely that segmentation is closely related to syntactic (as opposed to lexical) structure. Short of using a full-scale parser on the input we could use the parts of speech (POS) of words as a more suitable representation from which to predict segment boundaries. Parts of speech should also generalize much better to contexts containing N-grams not observed in the training data (assuming the POS of the words involved is known). We were able to test this hypothesis by using the POS-tagged version of the Switchboard corpus. We built two models based on POS from this data. Model I had all words replaced by their POS labels during training and test, and also used turn boundary information. Model II also used POS labels, but retained the word identities of certain word classes that were deemed to be particularly relevant to segmentation. These retained words include filled pauses, conjunctions, and certain discourse markers such as “okay,” “so,” “well,” etc. Results are shown in Table 3. Table 3: Segmentation performance using POS information Model Recall Precision FA SER Word-based 76.9% 66.9%</abstract>
    <biblio></biblio>
  </article>
  <article>
    <preamble>Torres.txt</preamble>
    <titre>Summary Evaluation</titre>
    <auteur></auteur>
    <abstract>the evaluation of text summarization systems without human models which is used to produce system rankings. The research is carried out using a new content-based evaluation framework called F RESA to compute a variety of divergences among probability distributions. We apply our comparison framework to various well-established content-based evaluation measures in text summarization such as C OVERAGE, R ESPONSIVENESS, P YRAMIDS and ROUGE studying their associations in various text summarization tasks including generic multi-document summarization in English and French, focus-based multi-document summarization in English and generic single-document summarization in French and Spanish. Index Terms—Text summarization evaluation, content-based evaluation measures, divergences. I. I NTRODUCTION T EXT summarization evaluation has always been a complex and controversial issue in computational linguistics. In the last decade, significant advances have been made in this field as well as various evaluation measures have been designed. Two evaluation campaigns have been led by the U.S. agence DARPA. The first one, SUMMAC, ran from 1996 to 1998 under the auspices of the Tipster program [1], and the second one, entitled DUC (Document Understanding Conference) [2], was the main evaluation forum from 2000 until 2007. Nowadays, the Text Analysis Conference (TAC) [3] provides a forum for assessment of different information access technologies including text summarization. Evaluation in text summarization can be extrinsic or intrinsic [4]. In an extrinsic evaluation, the summaries are assessed in the context of an specific task carried out by a human or a machine. In an intrinsic evaluation, the summaries are evaluated in reference to some ideal model. SUMMAC was mainly extrinsic while DUC and TAC followed an intrinsic evaluation paradigm. In an intrinsic evaluation, an Manuscript received June 8, 2010. Manuscript accepted for publication July 25, 2010. Juan-Manuel Torres-Moreno is with LIA/Université d’Avignon, France and École Polytechnique de Montréal, Canada (juan-manuel.torres@univ-avignon.fr). Eric SanJuan is with LIA/Université d’Avignon, France (eric.sanjuan@univ-avignon.fr). Horacio Saggion is with DTIC/Universitat Pompeu Fabra, Spain (horacio.saggion@upf.edu). Iria da Cunha is with IULA/Universitat Pompeu Fabra, Spain; LIA/Université d’Avignon, France and Instituto de Ingenierı́a/UNAM, Mexico (iria.dacunha@upf.edu). Patricia Velázquez-Morales is with VM Labs, France (patricia velazquez@yahoo.com). 13 Polibits (42) 2010 Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales of models and the identification, matching, and weighting of SCUs in both: models and peers. [12] evaluated the effectiveness of the Jensen-Shannon (J S) [13] theoretic measure in predicting systems ranks in two summarization tasks: query-focused and update summarization. They have shown that ranks produced by P YRAMIDS and those produced by J S measure correlate. However, they did not investigate the effect of the measure in summarization tasks such as generic multi-document summarization (DUC 2004 Task 2), biographical summarization (DUC 2004 Task 5), opinion summarization (TAC 2008 OS), and summarization in languages other than English. In this paper we present a series of experiments aimed at a better understanding of the value of the J S divergence for ranking summarization systems. We have carried out experimentation with the proposed measure and we have verified that in certain tasks (such as those studied by [12]) there is a strong correlation among P YRAMIDS, R ESPONSIVENESS and the J S divergence, but as we will show in this paper, there are datasets in which the correlation is not so strong. We also present experiments in Spanish and French showing positive correlation between the J S and ROUGE which is the de facto evaluation measure used in evaluation of non-English summarization. To the best of our knowledge this is the more extensive set of experiments interpreting the value of evaluation without human models. The rest of the paper is organized in the following way: First in Section II we introduce related work in the area of content-based evaluation identifying the departing point for our inquiry; then in Section III we explain the methodology adopted in our work and the tools and resources used for experimentation. In Section IV we present the experiments carried out together with the results. Section V discusses the results and Section VI concludes the paper and identifies future work. non-random systems, no clear conclusion was reached on the value of each of the studied measures. Nowadays, a widespread summarization evaluation framework is ROUGE [14], which offers a set of statistics that compare peer summaries with models. It counts co-occurrences of n-grams in peer and models to derive a score. There are several statistics depending on the used n-grams and the text processing applied to the input texts (e.g., lemmatization, stop-word removal). [15] proposed a method of evaluation based on the use of “distances” or divergences between two probability distributions (the distribution of units in the automatic summary and the distribution of units in the model summary). They studied two different Information Theoretic measures of divergence: the Kullback-Leibler (KL) [16] and Jensen-Shannon (J S) [13] divergences. KL computes the divergence between probability distributions P and Q in the following way: Pw 1X Pw log2 (1) DKL (P ||Q) = 2 w Qw While J S divergence is defined as follows: 1X 2Pw 2Qw DJ S (P ||Q) = Pw log2 + Qw log2 2 w Pw + Qw Pw + Qw (2) These measures can be applied to the distribution of units in system summaries P and reference summaries Q. The value obtained may be used as a score for the system summary. The method has been tested by [15] over the DUC 2002 corpus for single and multi-document summarization tasks showing good correlation among divergence measures and both coverage and ROUGE rankings. [12] went even further and, as in [5], they proposed to compare directly the distribution of words in full documents with the distribution of words in automatic summaries to derive a content-based evaluation measure. They found a high correlation between rankings produced using models and rankings produced without models. This last work is the departing point for our inquiry into the value of measures that do not rely on human models. II. R ELATED W ORK One of the first works to use content-based measures in text summarization evaluation is due to [5], who presented an evaluation framework to compare rankings of summarization systems produced by recall and cosine-based measures. They showed that there was weak correlation among rankings produced by recall, but that content-based measures produce rankings which were strongly correlated. This put forward the idea of using directly the full document for comparison purposes in text summarization evaluation. [6] presented a set of evaluation measures based on the notion of vocabulary overlap including n-gram overlap, cosine similarity, and longest common subsequence, and they applied them to multi-document summarization in English and Chinese. However, they did not evaluate the performance of the measures in different summarization tasks. [7] also compared various evaluation measures based on vocabulary overlap. Although these measures were able to separate random from Polibits (42) 2010 III. M ETHODOLOGY The followed methodology in this paper mirrors the one adopted in past work (e.g. [5], [7], [12]). Given a particular summarization task T , p data points to be summarized p−1 with input material {Ii }i=0 (e.g. document(s), question(s), s−1 topic(s)), s peer summaries {SUMi,k }k=0 for input i, and m−1 m model summaries {MODELi,j }j=0 for input i, we will compare rankings of the s peer summaries produced by various evaluation measures. Some measures that we use compare summaries with n of the m models: MEASUREM (SUMi,k , {MODELi,j }n−1 j=0 ) 14 (3) 6) Generic single document summarization in French using the “Canadien French Sociological Articles” corpus from the journal Perspectives interdisciplinaires sur le travail et la santé (PISTES)6 . It contains 50 sociological articles in French, each one with its corresponding 7) Generic multi-document-summarization in French using data from the RPM27 corpus [18], 20 different themes thematic – 185,223 words. (4) where Ii0 is some subset of input Ii . The values produced by the measures for each summary SUMi,k are averaged for each system k = 0, . . . , s − 1 and these averages are used to produce a ranking. Rankings are then compared using Spearman Rank correlation [17] which is used to measure the degree of association between two variables whose values are used to rank objects. We have chosen to use this correlation to compare directly results to those presented in [12]. Computation of correlations is done using the Statistics-RankCorrelation-0.12 package1 , which computes the rank correlation between two vectors. We also verified the good conformity of the results with the correlation test of Kendall τ calculated with the statistical software R. The two nonparametric tests of Spearman and Kendall do not really stand out as the treatment of ex-æquo. The good correspondence between the two tests shows that they do not introduce bias in our analysis. Subsequently will mention only the ρ of Sperman more widely used in this field. A. Tools We carry out experimentation using a new summarization evaluation framework: F RESA –FRamework for Evaluating Summaries Automatically–, which includes document-based summary evaluation measures based on probabilities distribution2 . As in the ROUGE package, F RESA supports different n-grams and skip n-grams probability distributions. The F RESA environment can be used in the evaluation of summaries in English, French, Spanish and Catalan, and it integrates filtering and lemmatization in the treatment of summaries and documents. It is developed in Perl and will be made publicly available. We also use the ROUGE package [10] to compute various ROUGE statistics in new datasets. For experimentation in the TAC and the DUC datasets we use directly the peer summaries produced by systems participating in the evaluations. For experimentation in Spanish and French (single and multi-document summarization) we have created summaries at a similar ratio to those of reference using the following systems: – ENERTEX [19], a summarizer based on a theory of textual energy; – CORTEX [20], a single-document sentence extraction system for Spanish and French that combines various statistical measures of relevance (angle between sentence and topic, various Hamming weights for sentences, etc.) and applies an optimal decision algorithm for sentence selection; – SUMMTERM [21], a terminology-based summarizer that is used for summarization of medical articles and uses specialized terminology for scoring and ranking sentences; – REG [22], summarization system based on an greedy algorithm; B. Summarization Tasks and Data Sets We have conducted our experimentation with the following summarization tasks and data sets: 1) Generic multi-document-summarization in English (production of a short summary of a cluster of related documents) using data from DUC’043 , task 2: 50 clusters, 10 documents each – 294,636 words. 2) Focused-based summarization in English (production of a short focused multi-document summary focused on the question “who is X?”, where X is a person’s name) using data from the DUC’04 task 5: 50 clusters, 10 documents each plus a target person name – 284,440 words. 4 http://www.nist.gov/tac/data/index.html written by the authors. As the experiments in [26] show, the professionals of a specialized domain (as, for example, the medical domain) adopt similar strategies to summarize their texts and they tend to choose roughly the same content chunks for their summaries. Previous studies have shown that author Because of this, the summary of the author of a medical article can be taken as reference for summaries evaluation. It is worth noting that there is still debate on the number of models to be used in summarization evaluation [28]. In the French corpus PISTES, we suspect the situation is similar to the Spanish case. p-value p < 0.005 p < 0.005 multi-document summarization in Spanish and French. In spite of the fact that the experiments for French and Spanish corpora use less data points (i.e., less summarizers per task) than for English, results are still quite significant. For DUC’04, we computed the J S measure for each peer summary in tasks 2 and 5 and we used J S, ROUGE, C OVERAGE and R ESPONSIVENESS scores to produce systems’ rankings. The various Spearman’s rank correlation values for DUC’04 are presented in Tables II (for task 2) and III (for task 5). For task 2, we have verified a strong correlation between J S and C OVERAGE. For task 5, the correlation between J S and C OVERAGE is weak, and that between J S and R ESPONSIVENESS is weak and negative. Although the Opinion Summarization (OS) task is a new type of summarization task and its evaluation is a complicated issue, we have decided to compare J S rankings with those obtained using P YRAMIDS and R ESPONSIVENESS in TAC’08. Spearman’s correlation values are listed in Table IV. As it can be seen, there is weak and negative correlation of J S with both P YRAMIDS and R ESPONSIVENESS. Correlation between P YRAMIDS and R ESPONSIVENESS rankings is high for this task (0.71 Spearman’s correlation value). For experimentation in mono-document summarization in Spanish and French, we have run 11 multi-lingual summarization systems; for experimentation in French, we have run 12 systems. In both cases, we have produced summaries at a compression rate close to the compression rate and ROUGE measures for each summary and we have averaged the measure’s values for each system. These averages were used to produce rankings per each measure. We computed Spearman’s correlations for all pairs of rankings. Results are presented in Tables V, VI and VII. All results show medium to strong correlation between the J S measures and ROUGE measures. However the J S measure based on uni-grams has lower correlation than J Ss which use n-grams of higher order. Note that table VII presents results for generic multi-document summarization in French, in this case correlation scores are lower than correlation scores for single-document summarization in French, a result which may be expected given the diversity of input in multi-document summarization. VI. C ONCLUSIONS AND F UTURE W ORK This paper has presented a series of experiments in content-based measures that do not rely on the use of model summaries for comparison purposes. We have carried out extensive experimentation with different summarization tasks drawing a clearer picture of tasks where the measures could be applied. This paper makes the following contributions: – We have shown that if we are only interested in ranking summarization systems according to the content of their automatic summaries, there are tasks were models could be subtituted by the full document in the computation of the J S measure obtaining reliable rankings. However, we have also found that the substitution of models by full-documents is not always advisable. We have V. D ISCUSSION The departing point for our inquiry into text summarization evaluation has been recent work on the use of content-based 17 Polibits (42) 2010 Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales TABLE II S PEARMAN ρ OF CONTENT- BASED MEASURES WITH C OVERAGE IN DUC’04 TASK 2 Mesure ROUGE-2 JS C OVERAGE 0.79 0.68 p-value p < 0.0050 p < 0.0025 TABLE III S PEARMAN ρ OF CONTENT- BASED MEASURES IN DUC’04 TASK 5 Mesure ROUGE-2 JS C OVERAGE 0.78 0.40 p-value p < 0.001 p < 0.050 R ESPONSIVENESS 0.44 -0.18 p-value p < 0.05 p < 0.25 TABLE IV S PEARMAN ρ OF CONTENT- BASED MEASURES IN TAC’08 OS TASK Mesure JS P YRAMIDS -0.13 p-value p < 0.25 R ESPONSIVENESS -0.14 p-value p < 0.25 TABLE V S PEARMAN ρ OF CONTENT- BASED MEASURES WITH ROUGE IN THE Medicina Clı́nica C ORPUS (S PANISH ) Mesure JS J S2 J S4 J SM ROUGE -1 0.56 0.88 0.88 0.82 p-value p < 0.100 p < 0.001 p < 0.001 p < 0.005 ROUGE -2 0.46 0.80 0.80 0.71 ROUGE -SU4 0.45 0.81 0.81 0.71 p-value p < 0.200 p < 0.005 p < 0.005 p < 0.010 a representation of the task/topic in the calculation of measures. To carry out these comparisons, however, we are Student Research Workshop. Toulouse, France: Association for Computational Linguistics, 9-11 July 2001 2001, pp. 49–54. [28] K. Owkzarzak and H. T. Dang, “Evaluation of automatic summaries: Metrics under varying data conditions,” in UCNLG+Sum’09, Suntec, Singapore, August 2009, pp. 23–30. [29] K. Knight and D. Marcu, “Statistics-based summarization-step one: Sentence compression,” in Proceedings of the National Conference on Artificial Intelligence. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999, 2000, pp. 703–710. [1] I. Mani, G. Klein, D. House, L. Hirschman, T. Firmin, and B. Sundheim, “Summac: a text summarization evaluation,” Natural Language Engineering, vol. 8, no. 1, pp. 43–68, 2002. [2] P. Over, H. Dang, and D. Harman, “DUC in context,” IPM, vol. 43, no. 6, pp. 1506–1520, 2007. [3] Proceedings of the Text Analysis Conference. Gaithesburg, Maryland, USA: NIST, November 17-19 2008. [4] K. Spärck Jones and J. Galliers, Evaluating Natural Language Processing Systems, An Analysis and Review, ser. Lecture Notes in Computer Science. Springer, 1996, vol. 1083. [5] R. L. Donaway, K. W. Drummey, and L. A. Mather, “A comparison of rankings produced by summarization evaluation measures,” in NAACL Workshop on Automatic Summarization, 2000, pp. 69–78. [6] H. Saggion, D. Radev, S. Teufel, and W. Lam, “Meta-evaluation of Summaries in a Cross-lingual Environment using Content-based Metrics,” in COLING 2002, Taipei, Taiwan, August 2002, pp. 849–855. [7] D. R. Radev, S. Teufel, H. Saggion, W. Lam, J. Blitzer, H. Qi, A. Çelebi, D. Liu, and E. Drábek, “Evaluation challenges in large-scale document summarization,” in ACL’03, 2003, pp. 375–382. [8] K. Papineni, S. Roukos, T. Ward, , and W. J. Zhu, “BLEU: a method for automatic evaluation of machine translation,” in ACL’02, 2002, pp. 311–318. [9] K. Pastra and H. Saggion, “Colouring summaries BLEU,” in Evaluation Initiatives in Natural Language Processing. Budapest, Hungary: EACL, 14 April 2003. [10] C.-Y. Lin, “ROUGE: A Package for Automatic Evaluation of Summaries,” in Text Summarization Branches Out: ACL-04 Workshop, M.-F. Moens and S. Szpakowicz, Eds., Barcelona, July 2004, pp. 74–81. [11] A. Nenkova and R. J. Passonneau, “Evaluating Content Selection in Summarization: The Pyramid Method,” in HLT-NAACL, 2004, pp. 145–152. [12] A. Louis and A. Nenkova, “Automatically Evaluating Content Selection in Summarization without Human Models,” in Empirical Methods in Natural Language Processing, Singapore, August 2009, pp. 306–314. [Online]. Available: http://www.aclweb.org/anthology/D/D09/D09-1032 [13] J. Lin, “Divergence Measures based on the Shannon Entropy,” IEEE Transactions on Information Theory, vol. 37, no. 145-151, 1991. [14] C.-Y. Lin and E. Hovy, “Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics,” in HLT-NAACL. Morristown, NJ, USA: Association for Computational Linguistics, 2003, pp. 71–78. [15] C.-Y. Lin, G. Cao, J. Gao, and J.-Y. Nie, “An information-theoretic approach to automatic evaluation of summaries,” in HLT-NAACL, Morristown, USA, 2006, pp. 463–470. [16] S. Kullback and R. Leibler, “On information and sufficiency,” Ann. of Math. Stat., vol. 22, no. 1, pp. 79–86, 1951. [17] S. Siegel and N. Castellan, Nonparametric Statistics for the Behavioral Sciences. McGraw-Hill, 1998. 19 Polibits (42) 2010</abstract>
    <biblio>Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales automatically generated summary (peer) has to be compared with one or more reference summaries (models). DUC used an interface called SEE to allow human judges to compare a peer with a model. Thus, judges give a C OVERAGE score to each peer produced by a system and the final system C OVERAGE score is the average of the C OVERAGE’s scores asigned. These system’s C OVERAGE scores can then be used to rank summarization systems. In the case of query-focused summarization (e.g. when the summary should answer a question or series of questions) a R ESPONSIVENESS score is also assigned to each summary, which indicates how responsive the summary is to the question(s). Because manual comparison of peer summaries with model summaries is an arduous and costly process, a body of research has been produced in the last decade on automatic content-based evaluation procedures. Early studies used text similarity measures such as cosine similarity (with or without weighting schema) to compare peer and model summaries [5]. Various vocabulary overlap measures such as n-grams overlap or longest common subsequence between peer and model have also been proposed [6], [7]. The B LEU machine translation evaluation measure [8] has also been tested in summarization [9]. The DUC conferences adopted the ROUGE package for content-based evaluation [10]. ROUGE implements a series of recall measures based on n-gram co-occurrence between a peer summary and a set of model summaries. These measures are used to produce systems’ rank. It has been shown that system rankings, produced by some ROUGE measures (e.g., ROUGE-2, which uses 2-grams), have a correlation with rankings produced using C OVERAGE. In recent years the P YRAMIDS evaluation method [11] has been introduced. It is based on the distribution of “content” of a set of model summaries. Summary Content Units (SCUs) are first identified in the model summaries, then each SCU receives a weight which is the number of models containing or expressing the same unit. Peer SCUs are identified in the peer, matched against model SCUs, and weighted accordingly. The P YRAMIDS score given to a peer is the ratio of the sum of the weights of its units and the sum of the weights of the best possible ideal summary with the same number of SCUs as the peer. The P YRAMIDS scores can be also used for ranking summarization systems. [11] showed that P YRAMIDS scores produced reliable system rankings when multiple (4 or more) models were used and that P YRAMIDS rankings correlate with rankings produced by ROUGE-2 and ROUGE-SU2 (i.e. ROUGE with skip 2-grams). However, this method requires the creation while other measures compare peers with all or some of the input material: MEASUREM (SUMi,k , Ii0 ) 3) Update-summarization task that consists of creating a summary out of a cluster of documents and a topic. Two sub-tasks are considered here: A) an initial summary has to be produced based on an initial set of documents and topic; B) an update summary has to be produced from a different (but related) cluster assuming documents used in A) are known. The English TAC’08 Update Summarization dataset is used, which consists of 48 topics with 20 documents each – 36,911 words. 4) Opinion summarization where systems have to analyze a set of blog articles and summarize the opinions about a target in the articles. The TAC’08 Opinion Summarization in English4 data set (taken from the Blogs06 Text Collection) is used: 25 clusters and targets (i.e., target entity and questions) were used – 1,167,735 words. 5) Generic single-document summarization in Spanish using the Medicina Clı́nica5 corpus, which is composed of 50 medical articles in Spanish, each one with its – R ESPONSIVENESS ranks summaries in a 5-point scale indicating how well the summary satisfied a given information need [2]. It is used in focused-based summarization tasks. This measure is used as indicated in equation 4 since a human judges the summary with respect to a given input “user need” (e.g., a question). R ESPONSIVENESS was used in DUC and TAC evaluations. – P YRAMIDS [11] is a content assessment measure which compares content units in a peer summary to weighted content units in a set of model summaries. This measure is used as indicated in equation 3 using human for content-based evaluation in the TAC evaluations. For DUC and TAC datasets the values of these measures are available and we used them directly. We used the following automatic evaluation measures in our experiments: – ROUGE [14], which is a recall metric that takes into account n-grams as units of content for comparing peer and model summaries. The ROUGE formula specified in [10] is as follows: ROUGE-n(R, M ) = P ∈ M count m n−gram∈P match (n − gram) P P count(n-gram) m ∈M Where P is the probability distribution of words w in text T and Q is the probability distribution of words w in summary S; N is the number of words in text and T summary N = NT +NS , B = 1.5|V |, Cw is the number S of words in the text and Cw is the number of words in the summary. For smoothing the summary’s probabilities we have used δ = 0.005. We have also implemented other smoothing approaches (e.g. Good-Turing [24], that uses the CPAN Perl’s Statistics-Smoothing-SGT-2.1.2 package11 ) in F RESA, but we do not use them in the experiments reported here. Following the ROUGE approach, in addition to word uni-grams we use 2-grams and skip n-grams computing divergences such as J S (using uni-grams) J S 2 (using 2-grams), J S 4 (using the skip n-grams of ROUGE-SU4), and J S M which is an average of the J S i . J Ss measures are used to compare a peer summary to its source document(s) in our framework (as indicated in equation 4). In the case of summarization of multiple documents, these are concatenated (in the given input order) to form a single input from which probabilities are computed. IV. E XPERIMENTS AND R ESULTS We first replicated the experiments presented in [12] to verify that our implementation of J S produced correlation results compatible with that work. We used the TAC’08 Update Summarization data set and computed J S and ROUGE measures for each peer summary. We produced two system rankings (one for each measure), which were compared to rankings produced using the manual P YRAMIDS and R ESPONSIVENESS scores. Spearman correlations were computed among the different rankings. The results are presented in Table I. These results confirm a high correlation among P YRAMIDS, R ESPONSIVENESS and J S. We also verified high correlation between J S and ROUGE-2 (0.83 Spearman correlation, not shown in the table) in this task and dataset. Then, we experimented with data from DUC’04, TAC’08 Opinion Summarization pilot task as well as single and P (5) where R is the summary to be evaluated, M is the set of model (human) summaries, countmatch is the number of common n-grams in m and P , and count is the number of n-grams in the model summaries. For the experiments 8 http://www.kryltech.com/summarizer.htm 9 http://www.pertinence.net 11 http://search.cpan.org/∼bjoernw/Statistics-Smoothing-SGT-2.1.2/ 10 http://www.copernic.com/en/products/summarizer Polibits (42) 2010 16 TABLE I S PEARMAN CORRELATION OF CONTENT- BASED MEASURES IN TAC’08 U PDATE S UMMARIZATION TASK Mesure ROUGE-2 JS P YRAMIDS 0.96 0.85 p-value p < 0.005 p < 0.005 R ESPONSIVENESS 0.92 0.74 evaluation metrics that do not rely on human models but that compare summary content to input content directly [12]. We have some positive and some negative results regarding the direct use of the full document in content-based evaluation. We have verified that in both generic muti-document summarization and in topic-based multi-document summarization in English correlation among measures that use human models (P YRAMIDS, R ESPONSIVENESS and ROUGE) and a measure that does not use models (J S divergence) is strong. We have found that correlation among the same measures is weak for summarization of biographical information and summarization of opinions in blogs. We believe that in these cases content-based measures should be considered, in addition to the input document, the summarization task (i.e. text-based representation, description) to better assess the content of the peers [25], the task being a determinant factor in the selection of content for the summary. Our multi-lingual experiments in generic single-document summarization confirm a strong correlation among the J S divergence and ROUGE measures. It is worth noting that ROUGE is in general the chosen framework for presenting content-based evaluation results in non-English summarization. For the experiments in Spanish, we are conscious that we only have one model summary to compare with the peers. F RESA will also be used in the new question-answer task campaign INEX’2010 (http://www.inex.otago.ac.nz/tracks/qa/ qa.asp) for the evaluation of long answers. This task aims to answer a question by extraction and agglomeration of sentences in Wikipedia. This kind of task corresponds to those for which we have found a high correlation among the measures J S and evaluation methods with human intervention. Moreover, the J S calculation will be among the summaries produced and a representative set of relevant passages from Wikipedia. F RESA will be used to compare three types of systems, although different tasks: the multi-document summarizer guided by a query, the search systems targeted information (focused IR) and the question answering systems. found weak correlation among different rankings in complex summarization tasks such as the summarization of biographical information and the summarization of opinions. – We have also carried out large-scale experiments in Spanish and French which show positive medium to strong correlation among system’s ranks produced by ROUGE and divergence measures that do not use the model summaries. – We have also presented a new framework, F RESA, for the computation of measures based on J S divergence. Following the ROUGE approach, F RESA package use word uni-grams, 2-grams and skip n-grams computing divergences. This framework will be available to the community for research purposes. Although we have made a number of contributions, this paper leaves many open questions than need to be addressed. In order to verify correlation between ROUGE and J S, in the short term we intend to extend our investigation to other languages such as Portuguese and Chinesse for which we have access to data and summarization technology. We also plan to apply F RESA to the rest of the DUC and TAC summarization tasks, by using several smoothing techniques. As a novel idea, we contemplate the possibility of adapting the evaluation framework for the phrase compression task [29], which, to our knowledge, does not have an efficient evaluation measure. The main idea is to calculate J S from an automatically-compressed sentence taking the complete sentence by reference. In the long term, we plan to incorporate Polibits (42) 2010 p-value p < 0.100 p < 0.002 p < 0.002 p < 0.020 ACKNOWLEDGMENT We are grateful to the Programa Ramón y Cajal from Ministerio de Ciencia e Innovación, Spain. This work is partially supported by: a postdoctoral grant from the National Program for Mobility of Research Human Resources (National Plan of Scientific Research, Development and Innovation 2008-2011, Ministerio de Ciencia e Innovación, Spain); the research project CONACyT, number 82050, and the research project PAPIIT-DGAPA (Universidad Nacional Autónoma de México), number IN403108. 18 TABLE VI S PEARMAN ρ OF CONTENT- BASED MEASURES WITH ROUGE IN THE PISTES C ORPUS (F RENCH ) Mesure JS J S2 J S4 J SM ROUGE -1 0.70 0.93 0.83 0.88 p-value p < 0.050 p < 0.002 p < 0.020 p < 0.010 ROUGE -2 0.73 0.86 0.76 0.83 p-value p < 0.05 p < 0.01 p < 0.05 p < 0.02 ROUGE -SU4 0.73 0.86 0.76 0.83 p-value p < 0.500 p < 0.005 p < 0.050 p < 0.010 TABLE VII S PEARMAN ρ OF CONTENT- BASED MEASURES WITH ROUGE IN THE RPM2 C ORPUS (F RENCH ) Measure JS J S2 J S4 J SM ROUGE -1 0.830 0.800 0.750 0.850 p-value p < 0.002 p < 0.005 p < 0.010 p < 0.002 ROUGE -2 0.660 0.590 0.520 0.640 R EFERENCES p-value p < 0.05 p < 0.05 p < 0.10 p < 0.05 ROUGE -SU4 0.741 0.680 0.620 0.740 p-value p < 0.01 p < 0.02 p < 0.05 p < 0.01 [18] C. de Loupy, M. Guégan, C. Ayache, S. Seng, and J.-M. Torres-Moreno, “A French Human Reference Corpus for multi-documents summarization and sentence compression,” in LREC’10, vol. 2, Malta, 2010, p. In press. [19] S. Fernandez, E. SanJuan, and J.-M. Torres-Moreno, “Textual Energy of Associative Memories: performants applications of Enertex algorithm in text summarization and topic segmentation,” in MICAI’07, 2007, pp. 861–871. [20] J.-M. Torres-Moreno, P. Velázquez-Morales, and J.-G. Meunier, “Condensés de textes par des méthodes numériques,” in JADT’02, vol. 2, St Malo, France, 2002, pp. 723–734. [21] J. Vivaldi, I. da Cunha, J.-M. Torres-Moreno, and P. Velázquez-Morales, “Automatic summarization using terminological and semantic resources,” in LREC’10, vol. 2, Malta, 2010, p. In press. [22] J.-M. Torres-Moreno and J. Ramirez, “REG : un algorithme glouton appliqué au résumé automatique de texte,” in JADT’10. Rome, 2010, p. In press. [23] V. Yatsko and T. Vishnyakov, “A method for evaluating modern systems of automatic text summarization,” Automatic Documentation and Mathematical Linguistics, vol. 41, no. 3, pp. 93–103, 2007. [24] C. D. Manning and H. Schütze, Foundations of Statistical Natural Language Processing. Cambridge, Massachusetts: The MIT Press, 1999. [25] K. Spärck Jones, “Automatic summarising: The state of the art,” IPM, vol. 43, no. 6, pp. 1449–1481, 2007. [26] I. da Cunha, L. Wanner, and M. T. Cabré, “Summarization of specialized discourse: The case of medical articles in spanish,” Terminology, vol. 13, no. 2, pp. 249–286, 2007.</biblio>
  </article>
  <article>
    <preamble>Word2Vec.txt</preamble>
    <titre>arXiv:1301.3781v3 [cs.CL] 7 Sep 2013 Efficient Estimation of Word Representations in Vector Space Tomas Mikolov Google Inc., Mountain View, CA</titre>
    <auteur>tmikolov@google.com Kai Chen Google Inc., Mountain View, CA kaichen@google.com Greg Corrado Google Inc., Mountain View, CA gcorrado@google.com Jeffrey Dean Google Inc., Mountain View, CA jeff@google.com</auteur>
    <abstract>We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities. 1</abstract>
    <biblio>[1] Y. Bengio, R. Ducharme, P. Vincent. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137-1155, 2003. [2] Y. Bengio, Y. LeCun. Scaling learning algorithms towards AI. In: Large-Scale Kernel Machines, MIT Press, 2007. [3] T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean. Large language models in machine translation. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Language Learning, 2007. [4] R. Collobert and J. Weston. A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. In International Conference on Machine Learning, ICML, 2008. [5] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu and P. Kuksa. Natural Language Processing (Almost) from Scratch. Journal of Machine Learning Research, 12:24932537, 2011. [6] J. Dean, G.S. Corrado, R. Monga, K. Chen, M. Devin, Q.V. Le, M.Z. Mao, M.A. Ranzato, A. Senior, P. Tucker, K. Yang, A. Y. Ng., Large Scale Distributed Deep Networks, NIPS, 2012. [7] J.C. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 2011. [8] J. Elman. Finding Structure in Time. Cognitive Science, 14, 179-211, 1990. [9] Eric H. Huang, R. Socher, C. D. Manning and Andrew Y. Ng. Improving Word Representations via Global Context and Multiple Word Prototypes. In: Proc. Association for Computational Linguistics, 2012. [10] G.E. Hinton, J.L. McClelland, D.E. Rumelhart. Distributed representations. In: Parallel distributed processing: Explorations in the microstructure of cognition. Volume 1: Foundations, MIT Press, 1986. [11] D.A. Jurgens, S.M. Mohammad, P.D. Turney, K.J. Holyoak. Semeval-2012 task 2: Measuring degrees of relational similarity. In: Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), 2012. [12] A.L. Maas, R.E. Daly, P.T. Pham, D. Huang, A.Y. Ng, and C. Potts. Learning word vectors for sentiment analysis. In Proceedings of ACL, 2011. [13] T. Mikolov. Language Modeling for Speech Recognition in Czech, Masters thesis, Brno University of Technology, 2007. [14] T. Mikolov, J. Kopecký, L. Burget, O. Glembek and J. Černocký. Neural network based language models for higly inflective languages, In: Proc. ICASSP 2009. [15] T. Mikolov, M. Karafiát, L. Burget, J. Černocký, S. Khudanpur. Recurrent neural network based language model, In: Proceedings of Interspeech, 2010. [16] T. Mikolov, S. Kombrink, L. Burget, J. Černocký, S. Khudanpur. Extensions of recurrent neural network language model, In: Proceedings of ICASSP 2011. [17] T. Mikolov, A. Deoras, S. Kombrink, L. Burget, J. Černocký. Empirical Evaluation and Combination of Advanced Language Modeling Techniques, In: Proceedings of Interspeech, 2011. 4 The code is available at https://code.google.com/p/word2vec/ 11 [18] T. Mikolov, A. Deoras, D. Povey, L. Burget, J. Černocký. Strategies for Training Large Scale Neural Network Language Models, In: Proc. Automatic Speech Recognition and Understanding, 2011. [19] T. Mikolov. Statistical Language Models based on Neural Networks. PhD thesis, Brno University of Technology, 2012. [20] T. Mikolov, W.T. Yih, G. Zweig. Linguistic Regularities in Continuous Space Word Representations. NAACL HLT 2013. [21] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed Representations of Words and Phrases and their Compositionality. Accepted to NIPS 2013. [22] A. Mnih, G. Hinton. Three new graphical models for statistical language modelling. ICML, 2007. [23] A. Mnih, G. Hinton. A Scalable Hierarchical Distributed Language Model. Advances in Neural Information Processing Systems 21, MIT Press, 2009. [24] A. Mnih, Y.W. Teh. A fast and simple algorithm for training neural probabilistic language models. ICML, 2012. [25] F. Morin, Y. Bengio. Hierarchical Probabilistic Neural Network Language Model. AISTATS, 2005. [26] D. E. Rumelhart, G. E. Hinton, R. J. Williams. Learning internal representations by backpropagating errors. Nature, 323:533.536, 1986. [27] H. Schwenk. Continuous space language models. Computer Speech and Language, vol. 21, 2007. [28] R. Socher, E.H. Huang, J. Pennington, A.Y. Ng, and C.D. Manning. Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection. In NIPS, 2011. [29] J. Turian, L. Ratinov, Y. Bengio. Word Representations: A Simple and General Method for Semi-Supervised Learning. In: Proc. Association for Computational Linguistics, 2010. [30] P. D. Turney. Measuring Semantic Similarity by Latent Relational Analysis. In: Proc. International Joint Conference on Artificial Intelligence, 2005. [31] A. Zhila, W.T. Yih, C. Meek, G. Zweig, T. Mikolov. Combining Heterogeneous Models for Measuring Relational Similarity. NAACL HLT 2013. [32] G. Zweig, C.J.C. Burges. The Microsoft Research Sentence Completion Challenge, Microsoft Research Technical Report MSR-TR-2011-129, 2011. 12</biblio>
  </article>
</articles>
